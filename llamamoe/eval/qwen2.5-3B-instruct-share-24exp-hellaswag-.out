W1007 03:48:15.314868 140344215974848 torch/distributed/run.py:757] 
W1007 03:48:15.314868 140344215974848 torch/distributed/run.py:757] *****************************************
W1007 03:48:15.314868 140344215974848 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1007 03:48:15.314868 140344215974848 torch/distributed/run.py:757] *****************************************
/root/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/root/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/root/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/root/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/root/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/root/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/root/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/root/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
using world size: 8, data-parallel size: 8, context-parallel size: 1, tensor-model-parallel size: 1, encoder-tensor-model-parallel size: 0, pipeline-model-parallel size: 1, encoder-pipeline-model-parallel size: 0
WARNING: overriding default arguments for no_load_rng:True                        with no_load_rng:True
WARNING: overriding default arguments for no_load_optim:True                        with no_load_optim:True
WARNING: overriding default arguments for micro_batch_size:1                        with micro_batch_size:4
WARNING: overriding default arguments for exit_on_missing_checkpoint:True                        with exit_on_missing_checkpoint:False
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  adaptive_seq_len ................................ True
  add_bias_linear ................................. False
  add_position_embedding .......................... False
  add_qkv_bias .................................... True
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  align_grad_reduce ............................... True
  align_param_gather .............................. False
  app_tag_run_name ................................ None
  app_tag_run_version ............................. 0.0.0
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... False
  async_save ...................................... None
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.0
  attention_softmax_in_fp32 ....................... False
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ True
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  bsz ............................................. 8
  calculate_per_token_loss ........................ False
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_assume_constant_structure .................. False
  ckpt_convert_format ............................. None
  ckpt_convert_save ............................... None
  ckpt_convert_update_legacy_dist_opt_format ...... False
  ckpt_format ..................................... torch
  ckpt_fully_parallel_load ........................ False
  ckpt_fully_parallel_save ........................ True
  ckpt_fully_parallel_save_deprecated ............. False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  config_logger_dir ............................... 
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. True
  cross_entropy_loss_fusion ....................... False
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 8
  data_path ....................................... ['/root/work/huangxin/nanda/Megatron-LM/llamamoe/data/data-bin/elhutr_qwen2.5_30b_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  ddp_average_in_collective ....................... False
  ddp_bucket_size ................................. None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  decrease_batch_size_if_needed ................... False
  defer_embedding_wgrad_compute ................... False
  deprecated_use_mcore_models ..................... True
  deterministic_mode .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  dist_ckpt_format_deprecated ..................... None
  dist_ckpt_strictness ............................ assume_ok_unexpected
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 1000000
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_ft_package ............................... False
  enable_one_logger ............................... True
  enable_shared_expert ............................ True
  encoder_num_layers .............................. 36
  encoder_pipeline_model_parallel_size ............ 0
  encoder_seq_length .............................. 2048
  encoder_tensor_model_parallel_size .............. 0
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 1000
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 8
  ffn_hidden_size ................................. 11008
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 1024
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 2048
  hybrid_attention_ratio .......................... 0.0
  hybrid_mlp_ratio ................................ 0.0
  hybrid_override_pattern ......................... None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 128
  lazy_mpu_init ................................... None
  load ............................................ /root/work/huangxin/nanda/models/Qwen2.5-3B-Instruct-qwenmoe-mcore-exp24-TP1PP1EP8
  local_rank ...................................... 0
  log_interval .................................... 1
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  logging_level ................................... None
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 5e-05
  lr_decay_iters .................................. 5000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 300
  lr_warmup_samples ............................... 0
  lr_wsd_decay_iters .............................. None
  lr_wsd_decay_samples ............................ None
  lr_wsd_decay_style .............................. exponential
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 32768
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 0.0
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.001
  moe_expert_capacity_factor ...................... None
  moe_extended_tp ................................. False
  moe_ffn_hidden_size ............................. 1376
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_layer_recompute ............................. False
  moe_lpr_loss_coeff .............................. 0.001
  moe_lpr_stage ................................... 1
  moe_pad_expert_input_to_capacity ................ False
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_pre_softmax .......................... False
  moe_router_topk ................................. 8
  moe_token_dispatcher_type ....................... alltoall
  moe_token_drop_policy ........................... probs
  moe_z_loss_coeff ................................ None
  moelpr_train_embeddings ......................... False
  nccl_communicator_config_path ................... None
  no_load_optim ................................... True
  no_load_rng ..................................... True
  no_persist_layer_norm ........................... False
  no_save_optim ................................... True
  no_save_rng ..................................... True
  non_persistent_ckpt_type ........................ None
  non_persistent_global_ckpt_dir .................. None
  non_persistent_save_interval .................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... RMSNorm
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_dataset_builder_threads ..................... 1
  num_experts ..................................... 24
  num_fewshot ..................................... 10
  num_layers ...................................... 36
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 2
  num_workers ..................................... 2
  one_logger_async ................................ False
  one_logger_project .............................. megatron-lm
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. True
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ True
  overlap_param_gather_with_optimizer_step ........ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... rope
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  renormalize_blend_weights ....................... False
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  results_path .................................... /root/work/huangxin/nanda/Megatron-LM/llamamoe/eval/results/mcore_qwen2.5-3B-instruct-share-24exp-hellaswag_expanded.json
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_base ..................................... 1000000
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  s3_cache_path ................................... None
  sample_rate ..................................... 1.0
  save ............................................ /root/work/huangxin/nanda/models/Qwen2.5-3B-Instruct-qwenmoe-mcore-exp24-TP1PP1EP8
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  skipped_train_samples ........................... 0
  spec ............................................ None
  split ........................................... 99990,8,2
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  task_list ....................................... hellaswag_el,hellaswag_hu,hellaswag_tr
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  tiktoken_num_special_tokens ..................... 1000
  tiktoken_pattern ................................ None
  tiktoken_special_tokens ......................... None
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. /root/work/huangxin/nanda/models/Qwen/Qwen2.5-3B-Instruct
  tokenizer_type .................................. HuggingFaceTokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 5000
  train_samples ................................... None
  train_sync_interval ............................. None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... False
  use_dist_ckpt_deprecated ........................ False
  use_distributed_optimizer ....................... True
  use_flash_attn .................................. True
  use_legacy_models ............................... False
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. True
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... None
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  wgrad_deferral_limit ............................ 0
  world_size ...................................... 8
  yaml_cfg ........................................ None
  zero_expert_down_proj ........................... False
-------------------- end of arguments ---------------------
> building HuggingFaceTokenizer tokenizer ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
 > padded vocab (size: 151936) with 0 dummy tokens (new size: 151936)
> initializing torch distributed ...
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/root/work/huangxin/nanda/Megatron-LM/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/root/work/huangxin/nanda/Megatron-LM/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.025 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
>>> done with compiling and loading fused kernels. Compilation time: 0.009 seconds
[INFO     | lm-eval            ]: `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.
[INFO     | lm-eval            ]: `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.
[INFO     | lm-eval            ]: `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.
[INFO     | lm-eval            ]: `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.
[INFO     | lm-eval            ]: `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.
[INFO     | lm-eval            ]: `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.
[INFO     | lm-eval            ]: `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.
[INFO     | lm-eval            ]: `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.
building GPT model ...
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[Rank 7] trainable params: 0 || all params: 4,000,903,168 || trainable%: 0.0000
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
GPTModel(
  (embedding): LanguageModelEmbedding(
    (word_embeddings): VocabParallelEmbedding()
    (embedding_dropout): Dropout(p=0.0, inplace=False)
  )
  (rotary_pos_emb): RotaryEmbedding()
  (decoder): TransformerBlock(
    (layers): ModuleList(
      (0-35): 36 x TransformerLayer(
        (input_layernorm): IdentityOp()
        (self_attention): SelfAttention(
          (core_attention): TEDotProductAttention(
            (flash_attention): FlashAttention()
            (fused_attention): FusedAttention()
            (unfused_attention): UnfusedDotProductAttention(
              (scale_mask_softmax): FusedScaleMaskSoftmax()
              (attention_dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (linear_proj): TERowParallelLinear()
          (linear_qkv): TELayerNormColumnParallelLinear()
          (q_layernorm): IdentityOp()
          (k_layernorm): IdentityOp()
        )
        (pre_cross_attn_layernorm): IdentityOp()
        (cross_attention): IdentityOp()
        (cross_attn_bda): IdentityFuncOp()
        (pre_mlp_layernorm): RMSNorm()
        (mlp): MoELayer(
          (router): TopKRouter()
          (shared_expert_gate): LanguageRouter()
          (shared_expert): MLP(
            (linear_fc1): ColumnParallelLinear()
            (linear_fc2): RowParallelLinear()
          )
          (experts): SequentialMLP(
            (local_experts): ModuleList(
              (0-2): 3 x MLP(
                (linear_fc1): ColumnParallelLinear()
                (linear_fc2): RowParallelLinear()
              )
            )
          )
        )
      )
    )
    (final_layernorm): RMSNorm()
  )
  (output_layer): ColumnParallelLinear()
)
Fine-tuning method: MoE-LPR stage 1
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[Rank 1] trainable params: 0 || all params: 4,000,903,168 || trainable%: 0.0000
[Rank 5] trainable params: 0 || all params: 4,000,903,168 || trainable%: 0.0000
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context
[Rank 3] trainable params: 0 || all params: 4,000,903,168 || trainable%: 0.0000
[Rank 6] trainable params: 0 || all params: 4,000,903,168 || trainable%: 0.0000
[Rank 0] trainable params: 0 || all params: 4,000,903,168 || trainable%: 0.0000
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 4000903168
[Rank 2] trainable params: 0 || all params: 4,000,903,168 || trainable%: 0.0000
[Rank 4] trainable params: 0 || all params: 4,000,903,168 || trainable%: 0.0000
 loading checkpoint from /root/work/huangxin/nanda/models/Qwen2.5-3B-Instruct-qwenmoe-mcore-exp24-TP1PP1EP8 at iteration 12000
 checkpoint version 3.0
  successfully loaded checkpoint from /root/work/huangxin/nanda/models/Qwen2.5-3B-Instruct-qwenmoe-mcore-exp24-TP1PP1EP8 [ t 0, p 0 ] at iteration 12000
DistributedDataParallel(
  (module): Float16Module(
    (module): GPTModel(
      (embedding): LanguageModelEmbedding(
        (word_embeddings): VocabParallelEmbedding()
        (embedding_dropout): Dropout(p=0.0, inplace=False)
      )
      (rotary_pos_emb): RotaryEmbedding()
      (decoder): TransformerBlock(
        (layers): ModuleList(
          (0-35): 36 x TransformerLayer(
            (input_layernorm): IdentityOp()
            (self_attention): SelfAttention(
              (core_attention): TEDotProductAttention(
                (flash_attention): FlashAttention()
                (fused_attention): FusedAttention()
                (unfused_attention): UnfusedDotProductAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.0, inplace=False)
                )
              )
              (linear_proj): TERowParallelLinear()
              (linear_qkv): TELayerNormColumnParallelLinear()
              (q_layernorm): IdentityOp()
              (k_layernorm): IdentityOp()
            )
            (pre_cross_attn_layernorm): IdentityOp()
            (cross_attention): IdentityOp()
            (cross_attn_bda): IdentityFuncOp()
            (pre_mlp_layernorm): RMSNorm()
            (mlp): MoELayer(
              (router): TopKRouter()
              (shared_expert_gate): LanguageRouter()
              (shared_expert): MLP(
                (linear_fc1): ColumnParallelLinear()
                (linear_fc2): RowParallelLinear()
              )
              (experts): SequentialMLP(
                (local_experts): ModuleList(
                  (0-2): 3 x MLP(
                    (linear_fc1): ColumnParallelLinear()
                    (linear_fc2): RowParallelLinear()
                  )
                )
              )
            )
          )
        )
        (final_layernorm): RMSNorm()
      )
      (output_layer): ColumnParallelLinear()
    )
  )
)
DistributedDataParallel(
  (module): Float16Module(
    (module): GPTModel(
      (embedding): LanguageModelEmbedding(
        (word_embeddings): VocabParallelEmbedding()
        (embedding_dropout): Dropout(p=0.0, inplace=False)
      )
      (rotary_pos_emb): RotaryEmbedding()
      (decoder): TransformerBlock(
        (layers): ModuleList(
          (0-35): 36 x TransformerLayer(
            (input_layernorm): IdentityOp()
            (self_attention): SelfAttention(
              (core_attention): TEDotProductAttention(
                (flash_attention): FlashAttention()
                (fused_attention): FusedAttention()
                (unfused_attention): UnfusedDotProductAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.0, inplace=False)
                )
              )
              (linear_proj): TERowParallelLinear()
              (linear_qkv): TELayerNormColumnParallelLinear()
              (q_layernorm): IdentityOp()
              (k_layernorm): IdentityOp()
            )
            (pre_cross_attn_layernorm): IdentityOp()
            (cross_attention): IdentityOp()
            (cross_attn_bda): IdentityFuncOp()
            (pre_mlp_layernorm): RMSNorm()
            (mlp): MoELayer(
              (router): TopKRouter()
              (shared_expert_gate): LanguageRouter()
              (shared_expert): MLP(
                (linear_fc1): ColumnParallelLinear()
                (linear_fc2): RowParallelLinear()
              )
              (experts): SequentialMLP(
                (local_experts): ModuleList(
                  (0-2): 3 x MLP(
                    (linear_fc1): ColumnParallelLinear()
                    (linear_fc2): RowParallelLinear()
                  )
                )
              )
            )
          )
        )
        (final_layernorm): RMSNorm()
      )
      (output_layer): ColumnParallelLinear()
    )
  )
)
DistributedDataParallel(
  (module): Float16Module(
    (module): GPTModel(
      (embedding): LanguageModelEmbedding(
        (word_embeddings): VocabParallelEmbedding()
        (embedding_dropout): Dropout(p=0.0, inplace=False)
      )
      (rotary_pos_emb): RotaryEmbedding()
      (decoder): TransformerBlock(
        (layers): ModuleList(
          (0-35): 36 x TransformerLayer(
            (input_layernorm): IdentityOp()
            (self_attention): SelfAttention(
              (core_attention): TEDotProductAttention(
                (flash_attention): FlashAttention()
                (fused_attention): FusedAttention()
                (unfused_attention): UnfusedDotProductAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.0, inplace=False)
                )
              )
              (linear_proj): TERowParallelLinear()
              (linear_qkv): TELayerNormColumnParallelLinear()
              (q_layernorm): IdentityOp()
              (k_layernorm): IdentityOp()
            )
            (pre_cross_attn_layernorm): IdentityOp()
            (cross_attention): IdentityOp()
            (cross_attn_bda): IdentityFuncOp()
            (pre_mlp_layernorm): RMSNorm()
            (mlp): MoELayer(
              (router): TopKRouter()
              (shared_expert_gate): LanguageRouter()
              (shared_expert): MLP(
                (linear_fc1): ColumnParallelLinear()
                (linear_fc2): RowParallelLinear()
              )
              (experts): SequentialMLP(
                (local_experts): ModuleList(
                  (0-2): 3 x MLP(
                    (linear_fc1): ColumnParallelLinear()
                    (linear_fc2): RowParallelLinear()
                  )
                )
              )
            )
          )
        )
        (final_layernorm): RMSNorm()
      )
      (output_layer): ColumnParallelLinear()
    )
  )
)
DistributedDataParallel(
  (module): Float16Module(
    (module): GPTModel(
      (embedding): LanguageModelEmbedding(
        (word_embeddings): VocabParallelEmbedding()
        (embedding_dropout): Dropout(p=0.0, inplace=False)
      )
      (rotary_pos_emb): RotaryEmbedding()
      (decoder): TransformerBlock(
        (layers): ModuleList(
          (0-35): 36 x TransformerLayer(
            (input_layernorm): IdentityOp()
            (self_attention): SelfAttention(
              (core_attention): TEDotProductAttention(
                (flash_attention): FlashAttention()
                (fused_attention): FusedAttention()
                (unfused_attention): UnfusedDotProductAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.0, inplace=False)
                )
              )
              (linear_proj): TERowParallelLinear()
              (linear_qkv): TELayerNormColumnParallelLinear()
              (q_layernorm): IdentityOp()
              (k_layernorm): IdentityOp()
            )
            (pre_cross_attn_layernorm): IdentityOp()
            (cross_attention): IdentityOp()
            (cross_attn_bda): IdentityFuncOp()
            (pre_mlp_layernorm): RMSNorm()
            (mlp): MoELayer(
              (router): TopKRouter()
              (shared_expert_gate): LanguageRouter()
              (shared_expert): MLP(
                (linear_fc1): ColumnParallelLinear()
                (linear_fc2): RowParallelLinear()
              )
              (experts): SequentialMLP(
                (local_experts): ModuleList(
                  (0-2): 3 x MLP(
                    (linear_fc1): ColumnParallelLinear()
                    (linear_fc2): RowParallelLinear()
                  )
                )
              )
            )
          )
        )
        (final_layernorm): RMSNorm()
      )
      (output_layer): ColumnParallelLinear()
    )
  )
)
DistributedDataParallel(
  (module): Float16Module(
    (module): GPTModel(
      (embedding): LanguageModelEmbedding(
        (word_embeddings): VocabParallelEmbedding()
        (embedding_dropout): Dropout(p=0.0, inplace=False)
      )
      (rotary_pos_emb): RotaryEmbedding()
      (decoder): TransformerBlock(
        (layers): ModuleList(
          (0-35): 36 x TransformerLayer(
            (input_layernorm): IdentityOp()
            (self_attention): SelfAttention(
              (core_attention): TEDotProductAttention(
                (flash_attention): FlashAttention()
                (fused_attention): FusedAttention()
                (unfused_attention): UnfusedDotProductAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.0, inplace=False)
                )
              )
              (linear_proj): TERowParallelLinear()
              (linear_qkv): TELayerNormColumnParallelLinear()
              (q_layernorm): IdentityOp()
              (k_layernorm): IdentityOp()
            )
            (pre_cross_attn_layernorm): IdentityOp()
            (cross_attention): IdentityOp()
            (cross_attn_bda): IdentityFuncOp()
            (pre_mlp_layernorm): RMSNorm()
            (mlp): MoELayer(
              (router): TopKRouter()
              (shared_expert_gate): LanguageRouter()
              (shared_expert): MLP(
                (linear_fc1): ColumnParallelLinear()
                (linear_fc2): RowParallelLinear()
              )
              (experts): SequentialMLP(
                (local_experts): ModuleList(
                  (0-2): 3 x MLP(
                    (linear_fc1): ColumnParallelLinear()
                    (linear_fc2): RowParallelLinear()
                  )
                )
              )
            )
          )
        )
        (final_layernorm): RMSNorm()
      )
      (output_layer): ColumnParallelLinear()
    )
  )
)
[INFO     | lm-eval            ]: Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
[INFO     | lm-eval            ]: Using pre-initialized model
[INFO     | lm-eval            ]: Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
[INFO     | lm-eval            ]: Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
[INFO     | lm-eval            ]: Using pre-initialized model
[INFO     | lm-eval            ]: Using pre-initialized model
DistributedDataParallel(
  (module): Float16Module(
    (module): GPTModel(
      (embedding): LanguageModelEmbedding(
        (word_embeddings): VocabParallelEmbedding()
        (embedding_dropout): Dropout(p=0.0, inplace=False)
      )
      (rotary_pos_emb): RotaryEmbedding()
      (decoder): TransformerBlock(
        (layers): ModuleList(
          (0-35): 36 x TransformerLayer(
            (input_layernorm): IdentityOp()
            (self_attention): SelfAttention(
              (core_attention): TEDotProductAttention(
                (flash_attention): FlashAttention()
                (fused_attention): FusedAttention()
                (unfused_attention): UnfusedDotProductAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.0, inplace=False)
                )
              )
              (linear_proj): TERowParallelLinear()
              (linear_qkv): TELayerNormColumnParallelLinear()
              (q_layernorm): IdentityOp()
              (k_layernorm): IdentityOp()
            )
            (pre_cross_attn_layernorm): IdentityOp()
            (cross_attention): IdentityOp()
            (cross_attn_bda): IdentityFuncOp()
            (pre_mlp_layernorm): RMSNorm()
            (mlp): MoELayer(
              (router): TopKRouter()
              (shared_expert_gate): LanguageRouter()
              (shared_expert): MLP(
                (linear_fc1): ColumnParallelLinear()
                (linear_fc2): RowParallelLinear()
              )
              (experts): SequentialMLP(
                (local_experts): ModuleList(
                  (0-2): 3 x MLP(
                    (linear_fc1): ColumnParallelLinear()
                    (linear_fc2): RowParallelLinear()
                  )
                )
              )
            )
          )
        )
        (final_layernorm): RMSNorm()
      )
      (output_layer): ColumnParallelLinear()
    )
  )
)
[INFO     | lm-eval            ]: Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
[INFO     | lm-eval            ]: Using pre-initialized model
[INFO     | lm-eval            ]: Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
[INFO     | lm-eval            ]: Using pre-initialized model
DistributedDataParallel(
  (module): Float16Module(
    (module): GPTModel(
      (embedding): LanguageModelEmbedding(
        (word_embeddings): VocabParallelEmbedding()
        (embedding_dropout): Dropout(p=0.0, inplace=False)
      )
      (rotary_pos_emb): RotaryEmbedding()
      (decoder): TransformerBlock(
        (layers): ModuleList(
          (0-35): 36 x TransformerLayer(
            (input_layernorm): IdentityOp()
            (self_attention): SelfAttention(
              (core_attention): TEDotProductAttention(
                (flash_attention): FlashAttention()
                (fused_attention): FusedAttention()
                (unfused_attention): UnfusedDotProductAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.0, inplace=False)
                )
              )
              (linear_proj): TERowParallelLinear()
              (linear_qkv): TELayerNormColumnParallelLinear()
              (q_layernorm): IdentityOp()
              (k_layernorm): IdentityOp()
            )
            (pre_cross_attn_layernorm): IdentityOp()
            (cross_attention): IdentityOp()
            (cross_attn_bda): IdentityFuncOp()
            (pre_mlp_layernorm): RMSNorm()
            (mlp): MoELayer(
              (router): TopKRouter()
              (shared_expert_gate): LanguageRouter()
              (shared_expert): MLP(
                (linear_fc1): ColumnParallelLinear()
                (linear_fc2): RowParallelLinear()
              )
              (experts): SequentialMLP(
                (local_experts): ModuleList(
                  (0-2): 3 x MLP(
                    (linear_fc1): ColumnParallelLinear()
                    (linear_fc2): RowParallelLinear()
                  )
                )
              )
            )
          )
        )
        (final_layernorm): RMSNorm()
      )
      (output_layer): ColumnParallelLinear()
    )
  )
)
[INFO     | lm-eval            ]: Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
[INFO     | lm-eval            ]: Using pre-initialized model
[INFO     | lm-eval            ]: Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
[INFO     | lm-eval            ]: Using pre-initialized model
DistributedDataParallel(
  (module): Float16Module(
    (module): GPTModel(
      (embedding): LanguageModelEmbedding(
        (word_embeddings): VocabParallelEmbedding()
        (embedding_dropout): Dropout(p=0.0, inplace=False)
      )
      (rotary_pos_emb): RotaryEmbedding()
      (decoder): TransformerBlock(
        (layers): ModuleList(
          (0-35): 36 x TransformerLayer(
            (input_layernorm): IdentityOp()
            (self_attention): SelfAttention(
              (core_attention): TEDotProductAttention(
                (flash_attention): FlashAttention()
                (fused_attention): FusedAttention()
                (unfused_attention): UnfusedDotProductAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.0, inplace=False)
                )
              )
              (linear_proj): TERowParallelLinear()
              (linear_qkv): TELayerNormColumnParallelLinear()
              (q_layernorm): IdentityOp()
              (k_layernorm): IdentityOp()
            )
            (pre_cross_attn_layernorm): IdentityOp()
            (cross_attention): IdentityOp()
            (cross_attn_bda): IdentityFuncOp()
            (pre_mlp_layernorm): RMSNorm()
            (mlp): MoELayer(
              (router): TopKRouter()
              (shared_expert_gate): LanguageRouter()
              (shared_expert): MLP(
                (linear_fc1): ColumnParallelLinear()
                (linear_fc2): RowParallelLinear()
              )
              (experts): SequentialMLP(
                (local_experts): ModuleList(
                  (0-2): 3 x MLP(
                    (linear_fc1): ColumnParallelLinear()
                    (linear_fc2): RowParallelLinear()
                  )
                )
              )
            )
          )
        )
        (final_layernorm): RMSNorm()
      )
      (output_layer): ColumnParallelLinear()
    )
  )
)
[INFO     | lm-eval            ]: Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
[INFO     | lm-eval            ]: Using pre-initialized model
Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since ilsp/hellaswag_greek couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'default' at /root/work/huangxin/nanda/data/cache/ilsp___hellaswag_greek/default/0.0.0/ac871418b051baaf0fe8fbd5d5f335d5fa8a1d66 (last modified on Thu Sep 19 15:25:10 2024).
Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_tr from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_hu from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_el from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_tr from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_hu from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_el from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_tr from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_hu from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_el from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_tr from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_hu from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_el from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_tr from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_hu from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_el from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_tr from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_hu from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_el from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_tr from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_hu from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_el from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
[WARNING  | datasets.load      ]: Using the latest cached version of the dataset since alexandrainst/m_hellaswag couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
[WARNING  | datasets.packaged_modules.cache.cache]: Found the latest cached dataset configuration 'hu' at /root/work/huangxin/nanda/data/cache/alexandrainst___m_hellaswag/hu/0.0.0/9d31dc982bd6285e081e3e3136332a38b9c1d7b7 (last modified on Thu Sep 19 15:25:21 2024).
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_tr from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_hu from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[WARNING  | lm-eval            ]: Overwriting default num_fewshot of hellaswag_el from None to 10
[INFO     | lm-eval            ]: Setting fewshot random generator seed to 1234
[INFO     | lm-eval            ]: Running loglikelihood requests
[INFO     | lm-eval            ]: Running loglikelihood requests
[INFO     | lm-eval            ]: Running loglikelihood requests
[INFO     | lm-eval            ]: Running loglikelihood requests
[INFO     | lm-eval            ]: Running loglikelihood requests
[INFO     | lm-eval            ]: Running loglikelihood requests
[INFO     | lm-eval            ]: Running loglikelihood requests
[INFO     | lm-eval            ]: Running loglikelihood requests
  0%|          | 0/14596 [00:00<?, ?it/s]/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
  0%|          | 8/14596 [00:15<7:50:23,  1.93s/it]  0%|          | 16/14596 [00:20<4:35:05,  1.13s/it]  0%|          | 24/14596 [00:24<3:37:53,  1.11it/s]  0%|          | 32/14596 [00:29<3:07:09,  1.30it/s]  0%|          | 40/14596 [00:34<2:53:53,  1.40it/s]  0%|          | 48/14596 [00:39<2:41:29,  1.50it/s]  0%|          | 56/14596 [00:43<2:35:17,  1.56it/s]  0%|          | 64/14596 [00:48<2:30:03,  1.61it/s]  0%|          | 72/14596 [00:53<2:29:40,  1.62it/s]  1%|          | 80/14596 [00:57<2:25:50,  1.66it/s]  1%|          | 88/14596 [01:02<2:25:36,  1.66it/s]  1%|          | 96/14596 [01:07<2:23:06,  1.69it/s]  1%|          | 104/14596 [01:12<2:24:26,  1.67it/s]  1%|          | 112/14596 [01:16<2:22:41,  1.69it/s]  1%|          | 120/14596 [01:21<2:25:17,  1.66it/s]  1%|          | 128/14596 [01:26<2:22:04,  1.70it/s]  1%|          | 136/14596 [01:31<2:24:36,  1.67it/s]  1%|          | 144/14596 [01:35<2:21:58,  1.70it/s]  1%|          | 152/14596 [01:40<2:23:16,  1.68it/s]  1%|          | 160/14596 [01:45<2:20:26,  1.71it/s]  1%|          | 168/14596 [01:49<2:21:30,  1.70it/s]  1%|          | 176/14596 [01:54<2:20:44,  1.71it/s]  1%|▏         | 184/14596 [01:59<2:21:55,  1.69it/s]  1%|▏         | 192/14596 [02:03<2:20:39,  1.71it/s]  1%|▏         | 200/14596 [02:08<2:22:14,  1.69it/s]  1%|▏         | 208/14596 [02:13<2:20:36,  1.71it/s]  1%|▏         | 216/14596 [02:18<2:21:50,  1.69it/s]  2%|▏         | 224/14596 [02:22<2:20:43,  1.70it/s]  2%|▏         | 232/14596 [02:27<2:21:21,  1.69it/s]  2%|▏         | 240/14596 [02:32<2:19:59,  1.71it/s]  2%|▏         | 248/14596 [02:37<2:22:05,  1.68it/s]  2%|▏         | 256/14596 [02:41<2:20:18,  1.70it/s]  2%|▏         | 264/14596 [02:46<2:21:48,  1.68it/s]  2%|▏         | 272/14596 [02:51<2:20:01,  1.70it/s]  2%|▏         | 280/14596 [02:55<2:21:12,  1.69it/s]  2%|▏         | 288/14596 [03:00<2:19:56,  1.70it/s]  2%|▏         | 296/14596 [03:05<2:21:12,  1.69it/s]  2%|▏         | 304/14596 [03:09<2:19:47,  1.70it/s]  2%|▏         | 312/14596 [03:14<2:21:13,  1.69it/s]  2%|▏         | 320/14596 [03:19<2:19:34,  1.70it/s]  2%|▏         | 328/14596 [03:24<2:21:22,  1.68it/s]  2%|▏         | 336/14596 [03:28<2:19:43,  1.70it/s]  2%|▏         | 344/14596 [03:33<2:21:02,  1.68it/s]  2%|▏         | 352/14596 [03:38<2:20:01,  1.70it/s]  2%|▏         | 360/14596 [03:43<2:21:37,  1.68it/s]  3%|▎         | 368/14596 [03:47<2:19:42,  1.70it/s]  3%|▎         | 376/14596 [03:52<2:20:39,  1.68it/s]  3%|▎         | 384/14596 [03:57<2:19:23,  1.70it/s]  3%|▎         | 392/14596 [04:02<2:21:07,  1.68it/s]  3%|▎         | 400/14596 [04:06<2:18:44,  1.71it/s]  3%|▎         | 408/14596 [04:11<2:19:56,  1.69it/s]  3%|▎         | 416/14596 [04:16<2:18:13,  1.71it/s]  3%|▎         | 424/14596 [04:21<2:20:10,  1.68it/s]  3%|▎         | 432/14596 [04:25<2:18:24,  1.71it/s]  3%|▎         | 440/14596 [04:30<2:19:13,  1.69it/s]  3%|▎         | 448/14596 [04:35<2:18:30,  1.70it/s]  3%|▎         | 456/14596 [04:39<2:19:07,  1.69it/s]  3%|▎         | 464/14596 [04:44<2:17:57,  1.71it/s]  3%|▎         | 472/14596 [04:49<2:19:50,  1.68it/s]  3%|▎         | 480/14596 [04:53<2:18:37,  1.70it/s]  3%|▎         | 488/14596 [04:58<2:19:09,  1.69it/s]  3%|▎         | 496/14596 [05:03<2:17:27,  1.71it/s]  3%|▎         | 504/14596 [05:08<2:18:26,  1.70it/s]  4%|▎         | 512/14596 [05:12<2:16:58,  1.71it/s]  4%|▎         | 520/14596 [05:17<2:19:11,  1.69it/s]  4%|▎         | 528/14596 [05:21<2:16:20,  1.72it/s]  4%|▎         | 536/14596 [05:26<2:19:06,  1.68it/s]  4%|▎         | 544/14596 [05:31<2:18:25,  1.69it/s]  4%|▍         | 552/14596 [05:36<2:18:47,  1.69it/s]  4%|▍         | 560/14596 [05:41<2:17:21,  1.70it/s]  4%|▍         | 568/14596 [05:45<2:19:01,  1.68it/s]  4%|▍         | 576/14596 [05:50<2:16:47,  1.71it/s]  4%|▍         | 584/14596 [05:55<2:17:52,  1.69it/s]  4%|▍         | 592/14596 [05:59<2:17:50,  1.69it/s]  4%|▍         | 600/14596 [06:04<2:19:04,  1.68it/s]  4%|▍         | 608/14596 [06:09<2:16:48,  1.70it/s]  4%|▍         | 616/14596 [06:14<2:17:59,  1.69it/s]  4%|▍         | 624/14596 [06:18<2:16:33,  1.71it/s]  4%|▍         | 632/14596 [06:23<2:17:32,  1.69it/s]  4%|▍         | 640/14596 [06:28<2:15:47,  1.71it/s]  4%|▍         | 648/14596 [06:32<2:17:04,  1.70it/s]  4%|▍         | 656/14596 [06:37<2:15:44,  1.71it/s]  5%|▍         | 664/14596 [06:42<2:17:13,  1.69it/s]  5%|▍         | 672/14596 [06:46<2:15:03,  1.72it/s]  5%|▍         | 680/14596 [06:51<2:17:15,  1.69it/s]  5%|▍         | 688/14596 [06:56<2:14:48,  1.72it/s]  5%|▍         | 696/14596 [07:01<2:16:30,  1.70it/s]  5%|▍         | 704/14596 [07:05<2:14:51,  1.72it/s]  5%|▍         | 712/14596 [07:10<2:17:01,  1.69it/s]  5%|▍         | 720/14596 [07:15<2:14:52,  1.71it/s]  5%|▍         | 728/14596 [07:19<2:16:00,  1.70it/s]  5%|▌         | 736/14596 [07:24<2:14:58,  1.71it/s]  5%|▌         | 744/14596 [07:29<2:16:28,  1.69it/s]  5%|▌         | 752/14596 [07:33<2:15:44,  1.70it/s]  5%|▌         | 760/14596 [07:38<2:16:18,  1.69it/s]  5%|▌         | 768/14596 [07:43<2:15:06,  1.71it/s]  5%|▌         | 776/14596 [07:48<2:15:52,  1.70it/s]  5%|▌         | 784/14596 [07:52<2:13:59,  1.72it/s]  5%|▌         | 792/14596 [07:57<2:14:58,  1.70it/s]  5%|▌         | 800/14596 [08:01<2:13:25,  1.72it/s]  6%|▌         | 808/14596 [08:06<2:15:11,  1.70it/s]  6%|▌         | 816/14596 [08:11<2:14:27,  1.71it/s]  6%|▌         | 824/14596 [08:16<2:14:54,  1.70it/s]  6%|▌         | 832/14596 [08:20<2:12:49,  1.73it/s]  6%|▌         | 840/14596 [08:25<2:14:16,  1.71it/s]  6%|▌         | 848/14596 [08:29<2:12:37,  1.73it/s]  6%|▌         | 856/14596 [08:34<2:13:43,  1.71it/s]  6%|▌         | 864/14596 [08:39<2:12:57,  1.72it/s]  6%|▌         | 872/14596 [08:44<2:13:21,  1.72it/s]  6%|▌         | 880/14596 [08:48<2:11:18,  1.74it/s]  6%|▌         | 888/14596 [08:53<2:13:51,  1.71it/s]  6%|▌         | 896/14596 [08:57<2:11:50,  1.73it/s]  6%|▌         | 904/14596 [09:02<2:13:46,  1.71it/s]  6%|▌         | 912/14596 [09:07<2:13:02,  1.71it/s]  6%|▋         | 920/14596 [09:12<2:14:38,  1.69it/s]  6%|▋         | 928/14596 [09:16<2:13:14,  1.71it/s]  6%|▋         | 936/14596 [09:21<2:14:00,  1.70it/s]  6%|▋         | 944/14596 [09:26<2:12:32,  1.72it/s]  7%|▋         | 952/14596 [09:30<2:13:34,  1.70it/s]  7%|▋         | 960/14596 [09:35<2:12:44,  1.71it/s]  7%|▋         | 968/14596 [09:40<2:14:05,  1.69it/s]  7%|▋         | 976/14596 [09:44<2:12:25,  1.71it/s]  7%|▋         | 984/14596 [09:49<2:14:38,  1.68it/s]  7%|▋         | 992/14596 [09:54<2:12:52,  1.71it/s]  7%|▋         | 1000/14596 [09:59<2:13:56,  1.69it/s]  7%|▋         | 1008/14596 [10:03<2:12:56,  1.70it/s]  7%|▋         | 1016/14596 [10:08<2:12:56,  1.70it/s]  7%|▋         | 1024/14596 [10:13<2:12:05,  1.71it/s]  7%|▋         | 1032/14596 [10:17<2:13:10,  1.70it/s]  7%|▋         | 1040/14596 [10:22<2:11:41,  1.72it/s]  7%|▋         | 1048/14596 [10:27<2:13:20,  1.69it/s]  7%|▋         | 1056/14596 [10:31<2:11:31,  1.72it/s]  7%|▋         | 1064/14596 [10:36<2:13:23,  1.69it/s]  7%|▋         | 1072/14596 [10:41<2:11:02,  1.72it/s]  7%|▋         | 1080/14596 [10:45<2:11:50,  1.71it/s]  7%|▋         | 1088/14596 [10:50<2:11:19,  1.71it/s]  8%|▊         | 1096/14596 [10:55<2:11:59,  1.70it/s]  8%|▊         | 1104/14596 [10:59<2:10:05,  1.73it/s]  8%|▊         | 1112/14596 [11:04<2:12:45,  1.69it/s]  8%|▊         | 1120/14596 [11:09<2:10:51,  1.72it/s]  8%|▊         | 1128/14596 [11:14<2:12:38,  1.69it/s]  8%|▊         | 1136/14596 [11:18<2:11:21,  1.71it/s]  8%|▊         | 1144/14596 [11:23<2:12:59,  1.69it/s]  8%|▊         | 1152/14596 [11:28<2:10:56,  1.71it/s]  8%|▊         | 1160/14596 [11:33<2:13:03,  1.68it/s]  8%|▊         | 1168/14596 [11:37<2:11:20,  1.70it/s]  8%|▊         | 1176/14596 [11:42<2:12:22,  1.69it/s]  8%|▊         | 1184/14596 [11:46<2:10:30,  1.71it/s]  8%|▊         | 1192/14596 [11:51<2:11:51,  1.69it/s]  8%|▊         | 1200/14596 [11:56<2:09:24,  1.73it/s]  8%|▊         | 1208/14596 [12:01<2:10:49,  1.71it/s]  8%|▊         | 1216/14596 [12:05<2:09:09,  1.73it/s]  8%|▊         | 1224/14596 [12:10<2:10:14,  1.71it/s]  8%|▊         | 1232/14596 [12:14<2:09:39,  1.72it/s]  8%|▊         | 1240/14596 [12:19<2:11:19,  1.70it/s]  9%|▊         | 1248/14596 [12:24<2:10:12,  1.71it/s]  9%|▊         | 1256/14596 [12:29<2:11:08,  1.70it/s]  9%|▊         | 1264/14596 [12:33<2:09:33,  1.71it/s]  9%|▊         | 1272/14596 [12:38<2:11:15,  1.69it/s]  9%|▉         | 1280/14596 [12:43<2:09:11,  1.72it/s]  9%|▉         | 1288/14596 [12:47<2:10:43,  1.70it/s]  9%|▉         | 1296/14596 [12:52<2:09:13,  1.72it/s]  9%|▉         | 1304/14596 [12:57<2:09:53,  1.71it/s]  9%|▉         | 1312/14596 [13:01<2:08:23,  1.72it/s]  9%|▉         | 1320/14596 [13:06<2:10:18,  1.70it/s]  9%|▉         | 1328/14596 [13:11<2:09:01,  1.71it/s]  9%|▉         | 1336/14596 [13:16<2:10:52,  1.69it/s]  9%|▉         | 1344/14596 [13:20<2:08:30,  1.72it/s]  9%|▉         | 1352/14596 [13:25<2:10:09,  1.70it/s]  9%|▉         | 1360/14596 [13:30<2:09:04,  1.71it/s]  9%|▉         | 1368/14596 [13:34<2:10:02,  1.70it/s]  9%|▉         | 1376/14596 [13:39<2:08:40,  1.71it/s]  9%|▉         | 1384/14596 [13:44<2:10:16,  1.69it/s] 10%|▉         | 1392/14596 [13:48<2:08:21,  1.71it/s] 10%|▉         | 1400/14596 [13:53<2:09:31,  1.70it/s] 10%|▉         | 1408/14596 [13:58<2:08:45,  1.71it/s] 10%|▉         | 1416/14596 [14:03<2:09:24,  1.70it/s] 10%|▉         | 1424/14596 [14:07<2:09:23,  1.70it/s] 10%|▉         | 1432/14596 [14:12<2:10:02,  1.69it/s] 10%|▉         | 1440/14596 [14:17<2:08:24,  1.71it/s] 10%|▉         | 1448/14596 [14:21<2:09:02,  1.70it/s] 10%|▉         | 1456/14596 [14:26<2:07:44,  1.71it/s] 10%|█         | 1464/14596 [14:31<2:09:22,  1.69it/s] 10%|█         | 1472/14596 [14:35<2:06:50,  1.72it/s] 10%|█         | 1480/14596 [14:40<2:08:24,  1.70it/s] 10%|█         | 1488/14596 [14:45<2:07:43,  1.71it/s] 10%|█         | 1496/14596 [14:49<2:08:38,  1.70it/s] 10%|█         | 1504/14596 [14:54<2:07:17,  1.71it/s] 10%|█         | 1512/14596 [14:59<2:07:57,  1.70it/s] 10%|█         | 1520/14596 [15:03<2:05:51,  1.73it/s] 10%|█         | 1528/14596 [15:08<2:07:23,  1.71it/s] 11%|█         | 1536/14596 [15:13<2:06:10,  1.73it/s] 11%|█         | 1544/14596 [15:17<2:07:50,  1.70it/s] 11%|█         | 1552/14596 [15:22<2:07:00,  1.71it/s] 11%|█         | 1560/14596 [15:27<2:07:48,  1.70it/s] 11%|█         | 1568/14596 [15:31<2:05:41,  1.73it/s] 11%|█         | 1576/14596 [15:36<2:07:04,  1.71it/s] 11%|█         | 1584/14596 [15:41<2:05:26,  1.73it/s] 11%|█         | 1592/14596 [15:45<2:06:57,  1.71it/s] 11%|█         | 1600/14596 [15:50<2:05:30,  1.73it/s] 11%|█         | 1608/14596 [15:55<2:06:46,  1.71it/s] 11%|█         | 1616/14596 [15:59<2:05:13,  1.73it/s] 11%|█         | 1624/14596 [16:04<2:06:43,  1.71it/s] 11%|█         | 1632/14596 [16:09<2:04:37,  1.73it/s] 11%|█         | 1640/14596 [16:13<2:06:42,  1.70it/s] 11%|█▏        | 1648/14596 [16:18<2:06:43,  1.70it/s] 11%|█▏        | 1656/14596 [16:23<2:07:03,  1.70it/s] 11%|█▏        | 1664/14596 [16:27<2:05:17,  1.72it/s] 11%|█▏        | 1672/14596 [16:32<2:06:16,  1.71it/s] 12%|█▏        | 1680/14596 [16:37<2:04:49,  1.72it/s] 12%|█▏        | 1688/14596 [16:41<2:06:02,  1.71it/s] 12%|█▏        | 1696/14596 [16:46<2:04:06,  1.73it/s] 12%|█▏        | 1704/14596 [16:51<2:04:46,  1.72it/s] 12%|█▏        | 1712/14596 [16:55<2:04:30,  1.72it/s] 12%|█▏        | 1720/14596 [17:00<2:05:33,  1.71it/s] 12%|█▏        | 1728/14596 [17:05<2:04:12,  1.73it/s] 12%|█▏        | 1736/14596 [17:09<2:06:15,  1.70it/s] 12%|█▏        | 1744/14596 [17:14<2:04:23,  1.72it/s] 12%|█▏        | 1752/14596 [17:19<2:05:38,  1.70it/s] 12%|█▏        | 1760/14596 [17:23<2:02:42,  1.74it/s] 12%|█▏        | 1768/14596 [17:28<2:04:36,  1.72it/s] 12%|█▏        | 1776/14596 [17:32<2:03:10,  1.73it/s] 12%|█▏        | 1784/14596 [17:37<2:04:36,  1.71it/s] 12%|█▏        | 1792/14596 [17:42<2:03:04,  1.73it/s] 12%|█▏        | 1800/14596 [17:46<2:04:01,  1.72it/s] 12%|█▏        | 1808/14596 [17:51<2:03:02,  1.73it/s] 12%|█▏        | 1816/14596 [17:56<2:04:55,  1.70it/s] 12%|█▏        | 1824/14596 [18:00<2:03:23,  1.73it/s] 13%|█▎        | 1832/14596 [18:05<2:04:01,  1.72it/s] 13%|█▎        | 1840/14596 [18:10<2:02:32,  1.73it/s] 13%|█▎        | 1848/14596 [18:14<2:03:47,  1.72it/s] 13%|█▎        | 1856/14596 [18:19<2:02:39,  1.73it/s] 13%|█▎        | 1864/14596 [18:24<2:03:03,  1.72it/s] 13%|█▎        | 1872/14596 [18:28<2:02:40,  1.73it/s] 13%|█▎        | 1880/14596 [18:33<2:02:27,  1.73it/s] 13%|█▎        | 1888/14596 [18:37<2:01:30,  1.74it/s] 13%|█▎        | 1896/14596 [18:42<2:02:57,  1.72it/s] 13%|█▎        | 1904/14596 [18:47<2:01:44,  1.74it/s] 13%|█▎        | 1912/14596 [18:51<2:02:51,  1.72it/s] 13%|█▎        | 1920/14596 [18:56<2:02:21,  1.73it/s] 13%|█▎        | 1928/14596 [19:01<2:03:08,  1.71it/s] 13%|█▎        | 1936/14596 [19:05<2:01:23,  1.74it/s] 13%|█▎        | 1944/14596 [19:10<2:03:39,  1.71it/s] 13%|█▎        | 1952/14596 [19:15<2:01:55,  1.73it/s] 13%|█▎        | 1960/14596 [19:19<2:03:13,  1.71it/s] 13%|█▎        | 1968/14596 [19:24<2:01:26,  1.73it/s] 14%|█▎        | 1976/14596 [19:29<2:03:06,  1.71it/s] 14%|█▎        | 1984/14596 [19:33<2:01:17,  1.73it/s] 14%|█▎        | 1992/14596 [19:38<2:02:50,  1.71it/s] 14%|█▎        | 2000/14596 [19:42<2:01:38,  1.73it/s] 14%|█▍        | 2008/14596 [19:47<2:02:56,  1.71it/s] 14%|█▍        | 2016/14596 [19:52<2:01:50,  1.72it/s] 14%|█▍        | 2024/14596 [19:57<2:02:51,  1.71it/s] 14%|█▍        | 2032/14596 [20:01<2:01:04,  1.73it/s] 14%|█▍        | 2040/14596 [20:06<2:02:29,  1.71it/s] 14%|█▍        | 2048/14596 [20:10<2:00:16,  1.74it/s] 14%|█▍        | 2056/14596 [20:15<2:01:30,  1.72it/s] 14%|█▍        | 2064/14596 [20:20<2:00:24,  1.73it/s] 14%|█▍        | 2072/14596 [20:24<2:01:41,  1.72it/s] 14%|█▍        | 2080/14596 [20:29<2:00:50,  1.73it/s] 14%|█▍        | 2088/14596 [20:34<2:01:45,  1.71it/s] 14%|█▍        | 2096/14596 [20:38<1:59:53,  1.74it/s] 14%|█▍        | 2104/14596 [20:43<2:01:21,  1.72it/s] 14%|█▍        | 2112/14596 [20:47<1:58:59,  1.75it/s] 15%|█▍        | 2120/14596 [20:52<2:00:45,  1.72it/s] 15%|█▍        | 2128/14596 [20:57<1:59:59,  1.73it/s] 15%|█▍        | 2136/14596 [21:01<2:00:13,  1.73it/s] 15%|█▍        | 2144/14596 [21:06<1:58:59,  1.74it/s] 15%|█▍        | 2152/14596 [21:10<1:59:00,  1.74it/s] 15%|█▍        | 2160/14596 [21:15<1:58:53,  1.74it/s] 15%|█▍        | 2168/14596 [21:20<2:00:07,  1.72it/s] 15%|█▍        | 2176/14596 [21:24<1:58:58,  1.74it/s] 15%|█▍        | 2184/14596 [21:29<2:00:23,  1.72it/s] 15%|█▌        | 2192/14596 [21:33<1:58:40,  1.74it/s] 15%|█▌        | 2200/14596 [21:38<2:00:45,  1.71it/s] 15%|█▌        | 2208/14596 [21:43<2:00:30,  1.71it/s] 15%|█▌        | 2216/14596 [21:48<2:00:59,  1.71it/s] 15%|█▌        | 2224/14596 [21:52<1:59:47,  1.72it/s] 15%|█▌        | 2232/14596 [21:57<2:00:26,  1.71it/s] 15%|█▌        | 2240/14596 [22:02<1:58:44,  1.73it/s] 15%|█▌        | 2248/14596 [22:06<1:59:34,  1.72it/s] 15%|█▌        | 2256/14596 [22:11<1:57:58,  1.74it/s] 16%|█▌        | 2264/14596 [22:15<1:58:41,  1.73it/s] 16%|█▌        | 2272/14596 [22:20<1:58:24,  1.73it/s] 16%|█▌        | 2280/14596 [22:25<1:59:32,  1.72it/s] 16%|█▌        | 2288/14596 [22:29<1:57:55,  1.74it/s] 16%|█▌        | 2296/14596 [22:34<1:59:22,  1.72it/s] 16%|█▌        | 2304/14596 [22:38<1:57:55,  1.74it/s] 16%|█▌        | 2312/14596 [22:43<1:59:34,  1.71it/s] 16%|█▌        | 2320/14596 [22:48<1:58:01,  1.73it/s] 16%|█▌        | 2328/14596 [22:53<1:59:14,  1.71it/s] 16%|█▌        | 2336/14596 [22:57<1:57:44,  1.74it/s] 16%|█▌        | 2344/14596 [23:02<1:58:46,  1.72it/s] 16%|█▌        | 2352/14596 [23:06<1:57:25,  1.74it/s] 16%|█▌        | 2360/14596 [23:11<1:58:28,  1.72it/s] 16%|█▌        | 2368/14596 [23:16<1:57:16,  1.74it/s] 16%|█▋        | 2376/14596 [23:20<1:58:13,  1.72it/s] 16%|█▋        | 2384/14596 [23:25<1:57:53,  1.73it/s] 16%|█▋        | 2392/14596 [23:30<1:58:12,  1.72it/s] 16%|█▋        | 2400/14596 [23:34<1:56:53,  1.74it/s] 16%|█▋        | 2408/14596 [23:39<1:58:29,  1.71it/s] 17%|█▋        | 2416/14596 [23:43<1:56:50,  1.74it/s] 17%|█▋        | 2424/14596 [23:48<1:58:16,  1.72it/s] 17%|█▋        | 2432/14596 [23:53<1:57:10,  1.73it/s] 17%|█▋        | 2440/14596 [23:57<1:57:42,  1.72it/s] 17%|█▋        | 2448/14596 [24:02<1:56:31,  1.74it/s] 17%|█▋        | 2456/14596 [24:07<1:57:21,  1.72it/s] 17%|█▋        | 2464/14596 [24:11<1:56:43,  1.73it/s] 17%|█▋        | 2472/14596 [24:16<1:57:53,  1.71it/s] 17%|█▋        | 2480/14596 [24:20<1:56:35,  1.73it/s] 17%|█▋        | 2488/14596 [24:25<1:58:06,  1.71it/s] 17%|█▋        | 2496/14596 [24:30<1:56:26,  1.73it/s] 17%|█▋        | 2504/14596 [24:35<1:57:26,  1.72it/s] 17%|█▋        | 2512/14596 [24:39<1:56:58,  1.72it/s] 17%|█▋        | 2520/14596 [24:44<1:57:41,  1.71it/s] 17%|█▋        | 2528/14596 [24:48<1:56:25,  1.73it/s] 17%|█▋        | 2536/14596 [24:53<1:57:23,  1.71it/s] 17%|█▋        | 2544/14596 [24:58<1:56:03,  1.73it/s] 17%|█▋        | 2552/14596 [25:03<1:57:32,  1.71it/s] 18%|█▊        | 2560/14596 [25:07<1:55:37,  1.73it/s] 18%|█▊        | 2568/14596 [25:12<1:57:41,  1.70it/s] 18%|█▊        | 2576/14596 [25:16<1:54:54,  1.74it/s] 18%|█▊        | 2584/14596 [25:21<1:56:36,  1.72it/s] 18%|█▊        | 2592/14596 [25:25<1:54:40,  1.74it/s] 18%|█▊        | 2600/14596 [25:30<1:55:56,  1.72it/s] 18%|█▊        | 2608/14596 [25:35<1:54:27,  1.75it/s] 18%|█▊        | 2616/14596 [25:39<1:55:19,  1.73it/s] 18%|█▊        | 2624/14596 [25:44<1:54:13,  1.75it/s] 18%|█▊        | 2632/14596 [25:49<1:56:27,  1.71it/s] 18%|█▊        | 2640/14596 [25:53<1:55:23,  1.73it/s] 18%|█▊        | 2648/14596 [25:58<1:56:56,  1.70it/s] 18%|█▊        | 2656/14596 [26:03<1:54:59,  1.73it/s] 18%|█▊        | 2664/14596 [26:07<1:56:56,  1.70it/s] 18%|█▊        | 2672/14596 [26:12<1:55:15,  1.72it/s] 18%|█▊        | 2680/14596 [26:17<1:56:55,  1.70it/s] 18%|█▊        | 2688/14596 [26:21<1:54:40,  1.73it/s] 18%|█▊        | 2696/14596 [26:26<1:56:14,  1.71it/s] 19%|█▊        | 2704/14596 [26:31<1:55:23,  1.72it/s] 19%|█▊        | 2712/14596 [26:35<1:55:33,  1.71it/s] 19%|█▊        | 2720/14596 [26:40<1:54:20,  1.73it/s] 19%|█▊        | 2728/14596 [26:45<1:55:35,  1.71it/s] 19%|█▊        | 2736/14596 [26:49<1:54:18,  1.73it/s] 19%|█▉        | 2744/14596 [26:54<1:55:41,  1.71it/s] 19%|█▉        | 2752/14596 [26:59<1:54:26,  1.72it/s] 19%|█▉        | 2760/14596 [27:03<1:55:22,  1.71it/s] 19%|█▉        | 2768/14596 [27:08<1:53:40,  1.73it/s] 19%|█▉        | 2776/14596 [27:13<1:54:41,  1.72it/s] 19%|█▉        | 2784/14596 [27:17<1:52:54,  1.74it/s] 19%|█▉        | 2792/14596 [27:22<1:54:36,  1.72it/s] 19%|█▉        | 2800/14596 [27:26<1:53:13,  1.74it/s] 19%|█▉        | 2808/14596 [27:31<1:54:12,  1.72it/s] 19%|█▉        | 2816/14596 [27:35<1:52:44,  1.74it/s] 19%|█▉        | 2824/14596 [27:40<1:54:25,  1.71it/s] 19%|█▉        | 2832/14596 [27:45<1:52:53,  1.74it/s] 19%|█▉        | 2840/14596 [27:50<1:54:08,  1.72it/s] 20%|█▉        | 2848/14596 [27:54<1:52:48,  1.74it/s] 20%|█▉        | 2856/14596 [27:59<1:54:08,  1.71it/s] 20%|█▉        | 2864/14596 [28:03<1:52:24,  1.74it/s] 20%|█▉        | 2872/14596 [28:08<1:53:38,  1.72it/s] 20%|█▉        | 2880/14596 [28:13<1:52:22,  1.74it/s] 20%|█▉        | 2888/14596 [28:17<1:53:47,  1.71it/s] 20%|█▉        | 2896/14596 [28:22<1:52:54,  1.73it/s] 20%|█▉        | 2904/14596 [28:27<1:53:38,  1.71it/s] 20%|█▉        | 2912/14596 [28:31<1:52:27,  1.73it/s] 20%|██        | 2920/14596 [28:36<1:53:58,  1.71it/s] 20%|██        | 2928/14596 [28:40<1:52:04,  1.74it/s] 20%|██        | 2936/14596 [28:45<1:53:40,  1.71it/s] 20%|██        | 2944/14596 [28:50<1:51:57,  1.73it/s] 20%|██        | 2952/14596 [28:55<1:53:11,  1.71it/s] 20%|██        | 2960/14596 [28:59<1:51:33,  1.74it/s] 20%|██        | 2968/14596 [29:04<1:52:00,  1.73it/s] 20%|██        | 2976/14596 [29:08<1:51:16,  1.74it/s] 20%|██        | 2984/14596 [29:13<1:53:00,  1.71it/s] 20%|██        | 2992/14596 [29:18<1:51:59,  1.73it/s] 21%|██        | 3000/14596 [29:22<1:51:37,  1.73it/s] 21%|██        | 3008/14596 [29:27<1:51:16,  1.74it/s] 21%|██        | 3016/14596 [29:32<1:52:35,  1.71it/s] 21%|██        | 3024/14596 [29:36<1:51:24,  1.73it/s] 21%|██        | 3032/14596 [29:41<1:52:03,  1.72it/s] 21%|██        | 3040/14596 [29:45<1:51:18,  1.73it/s] 21%|██        | 3048/14596 [29:50<1:51:59,  1.72it/s] 21%|██        | 3056/14596 [29:55<1:50:45,  1.74it/s] 21%|██        | 3064/14596 [29:59<1:52:24,  1.71it/s] 21%|██        | 3072/14596 [30:04<1:50:48,  1.73it/s] 21%|██        | 3080/14596 [30:09<1:51:48,  1.72it/s] 21%|██        | 3088/14596 [30:13<1:50:40,  1.73it/s] 21%|██        | 3096/14596 [30:18<1:52:37,  1.70it/s] 21%|██▏       | 3104/14596 [30:23<1:50:56,  1.73it/s] 21%|██▏       | 3112/14596 [30:27<1:51:17,  1.72it/s] 21%|██▏       | 3120/14596 [30:32<1:50:31,  1.73it/s] 21%|██▏       | 3128/14596 [30:36<1:50:45,  1.73it/s] 21%|██▏       | 3136/14596 [30:41<1:50:19,  1.73it/s] 22%|██▏       | 3144/14596 [30:46<1:51:31,  1.71it/s] 22%|██▏       | 3152/14596 [30:50<1:50:24,  1.73it/s] 22%|██▏       | 3160/14596 [30:55<1:50:47,  1.72it/s] 22%|██▏       | 3168/14596 [31:00<1:49:19,  1.74it/s] 22%|██▏       | 3176/14596 [31:04<1:51:06,  1.71it/s] 22%|██▏       | 3184/14596 [31:09<1:50:11,  1.73it/s] 22%|██▏       | 3192/14596 [31:14<1:50:53,  1.71it/s] 22%|██▏       | 3200/14596 [31:18<1:49:30,  1.73it/s] 22%|██▏       | 3208/14596 [31:23<1:50:40,  1.71it/s] 22%|██▏       | 3216/14596 [31:28<1:49:53,  1.73it/s] 22%|██▏       | 3224/14596 [31:32<1:50:47,  1.71it/s] 22%|██▏       | 3232/14596 [31:37<1:49:53,  1.72it/s] 22%|██▏       | 3240/14596 [31:42<1:50:44,  1.71it/s] 22%|██▏       | 3248/14596 [31:46<1:49:04,  1.73it/s] 22%|██▏       | 3256/14596 [31:51<1:50:40,  1.71it/s] 22%|██▏       | 3264/14596 [31:55<1:49:19,  1.73it/s] 22%|██▏       | 3272/14596 [32:00<1:50:14,  1.71it/s] 22%|██▏       | 3280/14596 [32:05<1:48:23,  1.74it/s] 23%|██▎       | 3288/14596 [32:10<1:50:19,  1.71it/s] 23%|██▎       | 3296/14596 [32:14<1:48:44,  1.73it/s] 23%|██▎       | 3304/14596 [32:19<1:51:10,  1.69it/s] 23%|██▎       | 3312/14596 [32:23<1:48:48,  1.73it/s] 23%|██▎       | 3320/14596 [32:28<1:50:37,  1.70it/s] 23%|██▎       | 3328/14596 [32:33<1:48:35,  1.73it/s] 23%|██▎       | 3336/14596 [32:37<1:49:16,  1.72it/s] 23%|██▎       | 3344/14596 [32:42<1:47:40,  1.74it/s] 23%|██▎       | 3352/14596 [32:47<1:49:35,  1.71it/s] 23%|██▎       | 3360/14596 [32:51<1:48:20,  1.73it/s] 23%|██▎       | 3368/14596 [32:56<1:48:43,  1.72it/s] 23%|██▎       | 3376/14596 [33:00<1:47:24,  1.74it/s] 23%|██▎       | 3384/14596 [33:05<1:48:30,  1.72it/s] 23%|██▎       | 3392/14596 [33:10<1:46:29,  1.75it/s] 23%|██▎       | 3400/14596 [33:14<1:48:32,  1.72it/s] 23%|██▎       | 3408/14596 [33:19<1:47:49,  1.73it/s] 23%|██▎       | 3416/14596 [33:24<1:48:34,  1.72it/s] 23%|██▎       | 3424/14596 [33:28<1:47:30,  1.73it/s] 24%|██▎       | 3432/14596 [33:33<1:48:43,  1.71it/s] 24%|██▎       | 3440/14596 [33:37<1:46:47,  1.74it/s] 24%|██▎       | 3448/14596 [33:42<1:48:18,  1.72it/s] 24%|██▎       | 3456/14596 [33:47<1:46:45,  1.74it/s] 24%|██▎       | 3464/14596 [33:52<1:48:54,  1.70it/s] 24%|██▍       | 3472/14596 [33:56<1:46:50,  1.74it/s] 24%|██▍       | 3480/14596 [34:01<1:47:56,  1.72it/s] 24%|██▍       | 3488/14596 [34:05<1:47:11,  1.73it/s] 24%|██▍       | 3496/14596 [34:10<1:48:13,  1.71it/s] 24%|██▍       | 3504/14596 [34:15<1:47:38,  1.72it/s] 24%|██▍       | 3512/14596 [34:20<1:49:09,  1.69it/s] 24%|██▍       | 3520/14596 [34:24<1:46:47,  1.73it/s] 24%|██▍       | 3528/14596 [34:29<1:48:35,  1.70it/s] 24%|██▍       | 3536/14596 [34:33<1:46:38,  1.73it/s] 24%|██▍       | 3544/14596 [34:38<1:48:07,  1.70it/s] 24%|██▍       | 3552/14596 [34:43<1:46:40,  1.73it/s] 24%|██▍       | 3560/14596 [34:48<1:47:20,  1.71it/s] 24%|██▍       | 3568/14596 [34:52<1:46:01,  1.73it/s] 24%|██▍       | 3576/14596 [34:57<1:46:43,  1.72it/s] 25%|██▍       | 3584/14596 [35:01<1:45:28,  1.74it/s] 25%|██▍       | 3592/14596 [35:06<1:46:49,  1.72it/s] 25%|██▍       | 3600/14596 [35:10<1:45:16,  1.74it/s] 25%|██▍       | 3608/14596 [35:15<1:47:00,  1.71it/s] 25%|██▍       | 3616/14596 [35:20<1:45:16,  1.74it/s] 25%|██▍       | 3624/14596 [35:25<1:46:49,  1.71it/s] 25%|██▍       | 3632/14596 [35:29<1:45:21,  1.73it/s] 25%|██▍       | 3640/14596 [35:34<1:46:59,  1.71it/s] 25%|██▍       | 3648/14596 [35:38<1:45:00,  1.74it/s] 25%|██▌       | 3656/14596 [35:43<1:46:42,  1.71it/s] 25%|██▌       | 3664/14596 [35:48<1:45:12,  1.73it/s] 25%|██▌       | 3672/14596 [35:53<1:46:32,  1.71it/s] 25%|██▌       | 3680/14596 [35:57<1:45:08,  1.73it/s] 25%|██▌       | 3688/14596 [36:02<1:45:45,  1.72it/s] 25%|██▌       | 3696/14596 [36:06<1:45:20,  1.72it/s] 25%|██▌       | 3704/14596 [36:11<1:46:00,  1.71it/s] 25%|██▌       | 3712/14596 [36:16<1:44:32,  1.74it/s] 25%|██▌       | 3720/14596 [36:20<1:46:05,  1.71it/s] 26%|██▌       | 3728/14596 [36:25<1:45:02,  1.72it/s] 26%|██▌       | 3736/14596 [36:30<1:45:19,  1.72it/s] 26%|██▌       | 3744/14596 [36:34<1:44:26,  1.73it/s] 26%|██▌       | 3752/14596 [36:39<1:45:25,  1.71it/s] 26%|██▌       | 3760/14596 [36:44<1:44:57,  1.72it/s] 26%|██▌       | 3768/14596 [36:48<1:45:34,  1.71it/s] 26%|██▌       | 3776/14596 [36:53<1:44:02,  1.73it/s] 26%|██▌       | 3784/14596 [36:58<1:45:16,  1.71it/s] 26%|██▌       | 3792/14596 [37:02<1:44:32,  1.72it/s] 26%|██▌       | 3800/14596 [37:07<1:45:29,  1.71it/s] 26%|██▌       | 3808/14596 [37:11<1:43:32,  1.74it/s] 26%|██▌       | 3816/14596 [37:16<1:44:57,  1.71it/s] 26%|██▌       | 3824/14596 [37:21<1:42:54,  1.74it/s] 26%|██▋       | 3832/14596 [37:25<1:44:06,  1.72it/s] 26%|██▋       | 3840/14596 [37:30<1:43:38,  1.73it/s] 26%|██▋       | 3848/14596 [37:35<1:44:04,  1.72it/s] 26%|██▋       | 3856/14596 [37:39<1:43:17,  1.73it/s] 26%|██▋       | 3864/14596 [37:44<1:43:42,  1.72it/s] 27%|██▋       | 3872/14596 [37:48<1:42:39,  1.74it/s] 27%|██▋       | 3880/14596 [37:53<1:42:59,  1.73it/s] 27%|██▋       | 3888/14596 [37:58<1:42:40,  1.74it/s] 27%|██▋       | 3896/14596 [38:02<1:43:23,  1.72it/s] 27%|██▋       | 3904/14596 [38:07<1:41:54,  1.75it/s] 27%|██▋       | 3912/14596 [38:12<1:43:34,  1.72it/s] 27%|██▋       | 3920/14596 [38:16<1:42:14,  1.74it/s] 27%|██▋       | 3928/14596 [38:21<1:43:59,  1.71it/s] 27%|██▋       | 3936/14596 [38:25<1:42:34,  1.73it/s] 27%|██▋       | 3944/14596 [38:30<1:43:07,  1.72it/s] 27%|██▋       | 3952/14596 [38:35<1:42:27,  1.73it/s] 27%|██▋       | 3960/14596 [38:39<1:43:36,  1.71it/s] 27%|██▋       | 3968/14596 [38:44<1:41:51,  1.74it/s] 27%|██▋       | 3976/14596 [38:49<1:42:45,  1.72it/s] 27%|██▋       | 3984/14596 [38:53<1:42:00,  1.73it/s] 27%|██▋       | 3992/14596 [38:58<1:43:19,  1.71it/s] 27%|██▋       | 4000/14596 [39:02<1:41:32,  1.74it/s] 27%|██▋       | 4008/14596 [39:07<1:42:24,  1.72it/s] 28%|██▊       | 4016/14596 [39:12<1:41:15,  1.74it/s] 28%|██▊       | 4024/14596 [39:16<1:42:08,  1.73it/s] 28%|██▊       | 4032/14596 [39:21<1:40:39,  1.75it/s] 28%|██▊       | 4040/14596 [39:26<1:41:38,  1.73it/s] 28%|██▊       | 4048/14596 [39:30<1:40:39,  1.75it/s] 28%|██▊       | 4056/14596 [39:35<1:41:46,  1.73it/s] 28%|██▊       | 4064/14596 [39:39<1:40:57,  1.74it/s] 28%|██▊       | 4072/14596 [39:44<1:41:13,  1.73it/s] 28%|██▊       | 4080/14596 [39:49<1:40:40,  1.74it/s] 28%|██▊       | 4088/14596 [39:53<1:41:46,  1.72it/s] 28%|██▊       | 4096/14596 [39:58<1:39:54,  1.75it/s] 28%|██▊       | 4104/14596 [40:02<1:41:28,  1.72it/s] 28%|██▊       | 4112/14596 [40:07<1:40:25,  1.74it/s] 28%|██▊       | 4120/14596 [40:12<1:41:00,  1.73it/s] 28%|██▊       | 4128/14596 [40:16<1:40:06,  1.74it/s] 28%|██▊       | 4136/14596 [40:21<1:41:12,  1.72it/s] 28%|██▊       | 4144/14596 [40:25<1:40:10,  1.74it/s] 28%|██▊       | 4152/14596 [40:30<1:40:52,  1.73it/s] 29%|██▊       | 4160/14596 [40:35<1:39:40,  1.74it/s] 29%|██▊       | 4168/14596 [40:40<1:41:30,  1.71it/s] 29%|██▊       | 4176/14596 [40:44<1:39:40,  1.74it/s] 29%|██▊       | 4184/14596 [40:49<1:40:11,  1.73it/s] 29%|██▊       | 4192/14596 [40:53<1:39:25,  1.74it/s] 29%|██▉       | 4200/14596 [40:58<1:40:35,  1.72it/s] 29%|██▉       | 4208/14596 [41:02<1:38:50,  1.75it/s] 29%|██▉       | 4216/14596 [41:07<1:40:16,  1.73it/s] 29%|██▉       | 4224/14596 [41:12<1:39:36,  1.74it/s] 29%|██▉       | 4232/14596 [41:16<1:40:45,  1.71it/s] 29%|██▉       | 4240/14596 [41:21<1:39:11,  1.74it/s] 29%|██▉       | 4248/14596 [41:26<1:40:22,  1.72it/s] 29%|██▉       | 4256/14596 [41:30<1:38:50,  1.74it/s] 29%|██▉       | 4264/14596 [41:35<1:39:56,  1.72it/s] 29%|██▉       | 4272/14596 [41:39<1:39:01,  1.74it/s] 29%|██▉       | 4280/14596 [41:44<1:40:00,  1.72it/s] 29%|██▉       | 4288/14596 [41:48<1:37:57,  1.75it/s] 29%|██▉       | 4296/14596 [41:53<1:38:44,  1.74it/s] 29%|██▉       | 4304/14596 [41:58<1:38:03,  1.75it/s] 30%|██▉       | 4312/14596 [42:02<1:38:45,  1.74it/s] 30%|██▉       | 4320/14596 [42:07<1:38:42,  1.73it/s] 30%|██▉       | 4328/14596 [42:12<1:39:08,  1.73it/s] 30%|██▉       | 4336/14596 [42:16<1:37:56,  1.75it/s] 30%|██▉       | 4344/14596 [42:21<1:38:42,  1.73it/s] 30%|██▉       | 4352/14596 [42:25<1:37:54,  1.74it/s] 30%|██▉       | 4360/14596 [42:30<1:39:32,  1.71it/s] 30%|██▉       | 4368/14596 [42:35<1:37:19,  1.75it/s] 30%|██▉       | 4376/14596 [42:39<1:38:13,  1.73it/s] 30%|███       | 4384/14596 [42:44<1:37:42,  1.74it/s] 30%|███       | 4392/14596 [42:49<1:39:02,  1.72it/s] 30%|███       | 4400/14596 [42:53<1:37:49,  1.74it/s] 30%|███       | 4408/14596 [42:58<1:38:17,  1.73it/s] 30%|███       | 4416/14596 [43:02<1:37:32,  1.74it/s] 30%|███       | 4424/14596 [43:07<1:38:03,  1.73it/s] 30%|███       | 4432/14596 [43:11<1:36:39,  1.75it/s] 30%|███       | 4440/14596 [43:16<1:38:11,  1.72it/s] 30%|███       | 4448/14596 [43:21<1:37:08,  1.74it/s] 31%|███       | 4456/14596 [43:26<1:38:13,  1.72it/s] 31%|███       | 4464/14596 [43:30<1:37:35,  1.73it/s] 31%|███       | 4472/14596 [43:35<1:38:36,  1.71it/s] 31%|███       | 4480/14596 [43:39<1:36:24,  1.75it/s] 31%|███       | 4488/14596 [43:44<1:38:18,  1.71it/s] 31%|███       | 4496/14596 [43:49<1:36:37,  1.74it/s] 31%|███       | 4504/14596 [43:53<1:37:49,  1.72it/s] 31%|███       | 4512/14596 [43:58<1:36:32,  1.74it/s] 31%|███       | 4520/14596 [44:02<1:37:02,  1.73it/s] 31%|███       | 4528/14596 [44:07<1:36:40,  1.74it/s] 31%|███       | 4536/14596 [44:12<1:36:57,  1.73it/s] 31%|███       | 4544/14596 [44:16<1:36:17,  1.74it/s] 31%|███       | 4552/14596 [44:21<1:37:56,  1.71it/s] 31%|███       | 4560/14596 [44:26<1:36:15,  1.74it/s] 31%|███▏      | 4568/14596 [44:30<1:37:54,  1.71it/s] 31%|███▏      | 4576/14596 [44:35<1:35:54,  1.74it/s] 31%|███▏      | 4584/14596 [44:40<1:37:05,  1.72it/s] 31%|███▏      | 4592/14596 [44:44<1:35:34,  1.74it/s] 32%|███▏      | 4600/14596 [44:49<1:37:35,  1.71it/s] 32%|███▏      | 4608/14596 [44:53<1:35:15,  1.75it/s] 32%|███▏      | 4616/14596 [44:58<1:36:47,  1.72it/s] 32%|███▏      | 4624/14596 [45:03<1:35:43,  1.74it/s] 32%|███▏      | 4632/14596 [45:07<1:37:27,  1.70it/s] 32%|███▏      | 4640/14596 [45:12<1:35:23,  1.74it/s] 32%|███▏      | 4648/14596 [45:17<1:36:20,  1.72it/s] 32%|███▏      | 4656/14596 [45:21<1:35:06,  1.74it/s] 32%|███▏      | 4664/14596 [45:26<1:36:13,  1.72it/s] 32%|███▏      | 4672/14596 [45:30<1:34:37,  1.75it/s] 32%|███▏      | 4680/14596 [45:35<1:36:08,  1.72it/s] 32%|███▏      | 4688/14596 [45:40<1:34:56,  1.74it/s] 32%|███▏      | 4696/14596 [45:44<1:36:33,  1.71it/s] 32%|███▏      | 4704/14596 [45:49<1:34:24,  1.75it/s] 32%|███▏      | 4712/14596 [45:54<1:35:18,  1.73it/s] 32%|███▏      | 4720/14596 [45:58<1:34:24,  1.74it/s] 32%|███▏      | 4728/14596 [46:03<1:35:08,  1.73it/s] 32%|███▏      | 4736/14596 [46:07<1:34:32,  1.74it/s] 33%|███▎      | 4744/14596 [46:12<1:35:00,  1.73it/s] 33%|███▎      | 4752/14596 [46:16<1:33:45,  1.75it/s] 33%|███▎      | 4760/14596 [46:21<1:34:58,  1.73it/s] 33%|███▎      | 4768/14596 [46:26<1:33:27,  1.75it/s] 33%|███▎      | 4776/14596 [46:30<1:34:36,  1.73it/s] 33%|███▎      | 4784/14596 [46:35<1:33:01,  1.76it/s] 33%|███▎      | 4792/14596 [46:40<1:34:23,  1.73it/s] 33%|███▎      | 4800/14596 [46:44<1:32:59,  1.76it/s] 33%|███▎      | 4808/14596 [46:49<1:34:25,  1.73it/s] 33%|███▎      | 4816/14596 [46:53<1:32:40,  1.76it/s] 33%|███▎      | 4824/14596 [46:58<1:33:35,  1.74it/s] 33%|███▎      | 4832/14596 [47:02<1:32:10,  1.77it/s] 33%|███▎      | 4840/14596 [47:07<1:33:27,  1.74it/s] 33%|███▎      | 4848/14596 [47:11<1:32:59,  1.75it/s] 33%|███▎      | 4856/14596 [47:16<1:34:16,  1.72it/s] 33%|███▎      | 4864/14596 [47:21<1:32:43,  1.75it/s] 33%|███▎      | 4872/14596 [47:25<1:33:40,  1.73it/s] 33%|███▎      | 4880/14596 [47:30<1:32:25,  1.75it/s] 33%|███▎      | 4888/14596 [47:35<1:33:34,  1.73it/s] 34%|███▎      | 4896/14596 [47:39<1:31:59,  1.76it/s] 34%|███▎      | 4904/14596 [47:44<1:32:59,  1.74it/s] 34%|███▎      | 4912/14596 [47:48<1:31:35,  1.76it/s] 34%|███▎      | 4920/14596 [47:53<1:33:07,  1.73it/s] 34%|███▍      | 4928/14596 [47:57<1:32:02,  1.75it/s] 34%|███▍      | 4936/14596 [48:02<1:32:47,  1.74it/s] 34%|███▍      | 4944/14596 [48:07<1:31:41,  1.75it/s] 34%|███▍      | 4952/14596 [48:11<1:32:25,  1.74it/s] 34%|███▍      | 4960/14596 [48:16<1:31:09,  1.76it/s] 34%|███▍      | 4968/14596 [48:20<1:32:49,  1.73it/s] 34%|███▍      | 4976/14596 [48:25<1:31:26,  1.75it/s] 34%|███▍      | 4984/14596 [48:29<1:31:44,  1.75it/s] 34%|███▍      | 4992/14596 [48:34<1:32:06,  1.74it/s] 34%|███▍      | 5000/14596 [48:39<1:32:24,  1.73it/s] 34%|███▍      | 5008/14596 [48:43<1:30:36,  1.76it/s] 34%|███▍      | 5016/14596 [48:48<1:31:20,  1.75it/s] 34%|███▍      | 5024/14596 [48:52<1:30:25,  1.76it/s] 34%|███▍      | 5032/14596 [48:57<1:31:45,  1.74it/s] 35%|███▍      | 5040/14596 [49:01<1:30:36,  1.76it/s] 35%|███▍      | 5048/14596 [49:06<1:31:46,  1.73it/s] 35%|███▍      | 5056/14596 [49:11<1:30:22,  1.76it/s] 35%|███▍      | 5064/14596 [49:15<1:31:31,  1.74it/s] 35%|███▍      | 5072/14596 [49:20<1:30:14,  1.76it/s] 35%|███▍      | 5080/14596 [49:25<1:31:17,  1.74it/s] 35%|███▍      | 5088/14596 [49:29<1:29:59,  1.76it/s] 35%|███▍      | 5096/14596 [49:34<1:31:12,  1.74it/s] 35%|███▍      | 5104/14596 [49:38<1:30:08,  1.75it/s] 35%|███▌      | 5112/14596 [49:43<1:31:21,  1.73it/s] 35%|███▌      | 5120/14596 [49:47<1:29:53,  1.76it/s] 35%|███▌      | 5128/14596 [49:52<1:30:39,  1.74it/s] 35%|███▌      | 5136/14596 [49:56<1:29:34,  1.76it/s] 35%|███▌      | 5144/14596 [50:01<1:30:47,  1.73it/s] 35%|███▌      | 5152/14596 [50:06<1:29:18,  1.76it/s] 35%|███▌      | 5160/14596 [50:10<1:30:23,  1.74it/s] 35%|███▌      | 5168/14596 [50:15<1:29:11,  1.76it/s] 35%|███▌      | 5176/14596 [50:19<1:30:03,  1.74it/s] 36%|███▌      | 5184/14596 [50:24<1:29:11,  1.76it/s] 36%|███▌      | 5192/14596 [50:29<1:30:14,  1.74it/s] 36%|███▌      | 5200/14596 [50:33<1:28:53,  1.76it/s] 36%|███▌      | 5208/14596 [50:38<1:30:05,  1.74it/s] 36%|███▌      | 5216/14596 [50:42<1:28:52,  1.76it/s] 36%|███▌      | 5224/14596 [50:47<1:29:35,  1.74it/s] 36%|███▌      | 5232/14596 [50:51<1:28:24,  1.77it/s] 36%|███▌      | 5240/14596 [50:56<1:29:22,  1.74it/s] 36%|███▌      | 5248/14596 [51:00<1:27:44,  1.78it/s] 36%|███▌      | 5256/14596 [51:05<1:28:47,  1.75it/s] 36%|███▌      | 5264/14596 [51:09<1:27:46,  1.77it/s] 36%|███▌      | 5272/14596 [51:14<1:29:06,  1.74it/s] 36%|███▌      | 5280/14596 [51:18<1:27:50,  1.77it/s] 36%|███▌      | 5288/14596 [51:23<1:28:48,  1.75it/s] 36%|███▋      | 5296/14596 [51:28<1:27:43,  1.77it/s] 36%|███▋      | 5304/14596 [51:32<1:28:29,  1.75it/s] 36%|███▋      | 5312/14596 [51:37<1:27:11,  1.77it/s] 36%|███▋      | 5320/14596 [51:41<1:28:27,  1.75it/s] 37%|███▋      | 5328/14596 [51:46<1:27:19,  1.77it/s] 37%|███▋      | 5336/14596 [51:51<1:28:40,  1.74it/s] 37%|███▋      | 5344/14596 [51:55<1:27:14,  1.77it/s] 37%|███▋      | 5352/14596 [52:00<1:28:02,  1.75it/s] 37%|███▋      | 5360/14596 [52:04<1:27:02,  1.77it/s] 37%|███▋      | 5368/14596 [52:09<1:28:07,  1.75it/s] 37%|███▋      | 5376/14596 [52:13<1:26:50,  1.77it/s] 37%|███▋      | 5384/14596 [52:18<1:27:57,  1.75it/s] 37%|███▋      | 5392/14596 [52:22<1:26:42,  1.77it/s]/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
 37%|███▋      | 5400/14596 [52:27<1:31:04,  1.68it/s]/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
 37%|███▋      | 5408/14596 [52:32<1:31:58,  1.66it/s]/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
 37%|███▋      | 5416/14596 [52:37<1:32:10,  1.66it/s]/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
 37%|███▋      | 5424/14596 [52:42<1:32:45,  1.65it/s] 37%|███▋      | 5432/14596 [52:47<1:31:55,  1.66it/s]/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
 37%|███▋      | 5440/14596 [52:52<1:32:21,  1.65it/s] 37%|███▋      | 5448/14596 [52:57<1:31:23,  1.67it/s] 37%|███▋      | 5456/14596 [53:01<1:29:05,  1.71it/s] 37%|███▋      | 5464/14596 [53:06<1:29:01,  1.71it/s] 37%|███▋      | 5472/14596 [53:10<1:27:16,  1.74it/s] 38%|███▊      | 5480/14596 [53:15<1:27:15,  1.74it/s] 38%|███▊      | 5488/14596 [53:19<1:26:07,  1.76it/s] 38%|███▊      | 5496/14596 [53:24<1:26:35,  1.75it/s] 38%|███▊      | 5504/14596 [53:28<1:25:14,  1.78it/s] 38%|███▊      | 5512/14596 [53:33<1:25:37,  1.77it/s] 38%|███▊      | 5520/14596 [53:37<1:24:42,  1.79it/s] 38%|███▊      | 5528/14596 [53:42<1:25:18,  1.77it/s] 38%|███▊      | 5536/14596 [53:46<1:24:07,  1.79it/s] 38%|███▊      | 5544/14596 [53:50<1:24:54,  1.78it/s] 38%|███▊      | 5552/14596 [53:55<1:23:27,  1.81it/s] 38%|███▊      | 5560/14596 [53:59<1:24:37,  1.78it/s] 38%|███▊      | 5568/14596 [54:04<1:23:15,  1.81it/s] 38%|███▊      | 5576/14596 [54:08<1:23:53,  1.79it/s] 38%|███▊      | 5584/14596 [54:12<1:22:42,  1.82it/s] 38%|███▊      | 5592/14596 [54:17<1:23:26,  1.80it/s] 38%|███▊      | 5600/14596 [54:21<1:22:31,  1.82it/s] 38%|███▊      | 5608/14596 [54:26<1:23:18,  1.80it/s] 38%|███▊      | 5616/14596 [54:30<1:22:17,  1.82it/s] 39%|███▊      | 5624/14596 [54:35<1:22:57,  1.80it/s] 39%|███▊      | 5632/14596 [54:39<1:21:53,  1.82it/s] 39%|███▊      | 5640/14596 [54:43<1:22:11,  1.82it/s] 39%|███▊      | 5648/14596 [54:48<1:21:33,  1.83it/s] 39%|███▉      | 5656/14596 [54:52<1:21:47,  1.82it/s] 39%|███▉      | 5664/14596 [54:56<1:20:59,  1.84it/s] 39%|███▉      | 5672/14596 [55:01<1:21:36,  1.82it/s] 39%|███▉      | 5680/14596 [55:05<1:20:38,  1.84it/s] 39%|███▉      | 5688/14596 [55:10<1:21:09,  1.83it/s] 39%|███▉      | 5696/14596 [55:14<1:20:21,  1.85it/s] 39%|███▉      | 5704/14596 [55:18<1:20:33,  1.84it/s] 39%|███▉      | 5712/14596 [55:22<1:20:07,  1.85it/s] 39%|███▉      | 5720/14596 [55:27<1:20:25,  1.84it/s] 39%|███▉      | 5728/14596 [55:31<1:19:46,  1.85it/s] 39%|███▉      | 5736/14596 [55:35<1:20:00,  1.85it/s] 39%|███▉      | 5744/14596 [55:40<1:19:18,  1.86it/s] 39%|███▉      | 5752/14596 [55:44<1:19:39,  1.85it/s] 39%|███▉      | 5760/14596 [55:48<1:19:03,  1.86it/s] 40%|███▉      | 5768/14596 [55:53<1:19:21,  1.85it/s] 40%|███▉      | 5776/14596 [55:57<1:18:56,  1.86it/s] 40%|███▉      | 5784/14596 [56:01<1:19:23,  1.85it/s] 40%|███▉      | 5792/14596 [56:05<1:18:38,  1.87it/s] 40%|███▉      | 5800/14596 [56:10<1:19:11,  1.85it/s] 40%|███▉      | 5808/14596 [56:14<1:18:21,  1.87it/s] 40%|███▉      | 5816/14596 [56:18<1:18:54,  1.85it/s] 40%|███▉      | 5824/14596 [56:23<1:18:20,  1.87it/s] 40%|███▉      | 5832/14596 [56:27<1:18:44,  1.86it/s] 40%|████      | 5840/14596 [56:31<1:18:16,  1.86it/s] 40%|████      | 5848/14596 [56:36<1:18:23,  1.86it/s] 40%|████      | 5856/14596 [56:40<1:17:46,  1.87it/s] 40%|████      | 5864/14596 [56:44<1:18:22,  1.86it/s] 40%|████      | 5872/14596 [56:48<1:17:44,  1.87it/s] 40%|████      | 5880/14596 [56:53<1:17:51,  1.87it/s] 40%|████      | 5888/14596 [56:57<1:17:15,  1.88it/s] 40%|████      | 5896/14596 [57:01<1:17:31,  1.87it/s] 40%|████      | 5904/14596 [57:05<1:17:01,  1.88it/s] 41%|████      | 5912/14596 [57:10<1:17:13,  1.87it/s] 41%|████      | 5920/14596 [57:14<1:16:40,  1.89it/s] 41%|████      | 5928/14596 [57:18<1:17:07,  1.87it/s] 41%|████      | 5936/14596 [57:22<1:16:28,  1.89it/s] 41%|████      | 5944/14596 [57:27<1:16:41,  1.88it/s] 41%|████      | 5952/14596 [57:31<1:16:10,  1.89it/s] 41%|████      | 5960/14596 [57:35<1:16:09,  1.89it/s] 41%|████      | 5968/14596 [57:39<1:15:44,  1.90it/s] 41%|████      | 5976/14596 [57:44<1:15:38,  1.90it/s] 41%|████      | 5984/14596 [57:48<1:15:21,  1.90it/s] 41%|████      | 5992/14596 [57:52<1:15:37,  1.90it/s] 41%|████      | 6000/14596 [57:56<1:15:12,  1.90it/s] 41%|████      | 6008/14596 [58:00<1:15:05,  1.91it/s] 41%|████      | 6016/14596 [58:04<1:14:28,  1.92it/s] 41%|████▏     | 6024/14596 [58:09<1:14:43,  1.91it/s] 41%|████▏     | 6032/14596 [58:13<1:14:04,  1.93it/s] 41%|████▏     | 6040/14596 [58:17<1:14:25,  1.92it/s] 41%|████▏     | 6048/14596 [58:21<1:13:54,  1.93it/s] 41%|████▏     | 6056/14596 [58:25<1:14:09,  1.92it/s] 42%|████▏     | 6064/14596 [58:29<1:14:08,  1.92it/s] 42%|████▏     | 6072/14596 [58:34<1:13:56,  1.92it/s] 42%|████▏     | 6080/14596 [58:38<1:13:52,  1.92it/s] 42%|████▏     | 6088/14596 [58:42<1:13:55,  1.92it/s] 42%|████▏     | 6096/14596 [58:46<1:13:30,  1.93it/s] 42%|████▏     | 6104/14596 [58:50<1:13:22,  1.93it/s] 42%|████▏     | 6112/14596 [58:54<1:13:27,  1.93it/s] 42%|████▏     | 6120/14596 [58:58<1:13:23,  1.92it/s] 42%|████▏     | 6128/14596 [59:03<1:13:26,  1.92it/s] 42%|████▏     | 6136/14596 [59:07<1:12:56,  1.93it/s] 42%|████▏     | 6144/14596 [59:11<1:12:52,  1.93it/s] 42%|████▏     | 6152/14596 [59:15<1:12:38,  1.94it/s] 42%|████▏     | 6160/14596 [59:19<1:12:40,  1.93it/s] 42%|████▏     | 6168/14596 [59:23<1:12:07,  1.95it/s] 42%|████▏     | 6176/14596 [59:27<1:11:52,  1.95it/s] 42%|████▏     | 6184/14596 [59:31<1:11:38,  1.96it/s] 42%|████▏     | 6192/14596 [59:35<1:11:50,  1.95it/s] 42%|████▏     | 6200/14596 [59:39<1:11:27,  1.96it/s] 43%|████▎     | 6208/14596 [59:44<1:11:30,  1.96it/s] 43%|████▎     | 6216/14596 [59:48<1:11:11,  1.96it/s] 43%|████▎     | 6224/14596 [59:52<1:11:04,  1.96it/s] 43%|████▎     | 6232/14596 [59:56<1:11:03,  1.96it/s] 43%|████▎     | 6240/14596 [1:00:00<1:10:48,  1.97it/s] 43%|████▎     | 6248/14596 [1:00:04<1:10:44,  1.97it/s] 43%|████▎     | 6256/14596 [1:00:08<1:10:49,  1.96it/s] 43%|████▎     | 6264/14596 [1:00:12<1:10:34,  1.97it/s] 43%|████▎     | 6272/14596 [1:00:16<1:10:23,  1.97it/s] 43%|████▎     | 6280/14596 [1:00:20<1:09:55,  1.98it/s] 43%|████▎     | 6288/14596 [1:00:24<1:10:01,  1.98it/s] 43%|████▎     | 6296/14596 [1:00:28<1:09:44,  1.98it/s] 43%|████▎     | 6304/14596 [1:00:32<1:09:47,  1.98it/s] 43%|████▎     | 6312/14596 [1:00:36<1:09:30,  1.99it/s] 43%|████▎     | 6320/14596 [1:00:40<1:09:30,  1.98it/s] 43%|████▎     | 6328/14596 [1:00:44<1:09:24,  1.99it/s] 43%|████▎     | 6336/14596 [1:00:48<1:09:32,  1.98it/s] 43%|████▎     | 6344/14596 [1:00:52<1:09:20,  1.98it/s] 44%|████▎     | 6352/14596 [1:00:56<1:09:21,  1.98it/s] 44%|████▎     | 6360/14596 [1:01:00<1:09:10,  1.98it/s] 44%|████▎     | 6368/14596 [1:01:04<1:09:03,  1.99it/s] 44%|████▎     | 6376/14596 [1:01:09<1:09:15,  1.98it/s] 44%|████▎     | 6384/14596 [1:01:13<1:09:00,  1.98it/s] 44%|████▍     | 6392/14596 [1:01:17<1:08:43,  1.99it/s] 44%|████▍     | 6400/14596 [1:01:21<1:08:47,  1.99it/s] 44%|████▍     | 6408/14596 [1:01:25<1:08:42,  1.99it/s] 44%|████▍     | 6416/14596 [1:01:29<1:08:48,  1.98it/s] 44%|████▍     | 6424/14596 [1:01:33<1:08:35,  1.99it/s] 44%|████▍     | 6432/14596 [1:01:37<1:08:32,  1.99it/s] 44%|████▍     | 6440/14596 [1:01:41<1:08:29,  1.98it/s] 44%|████▍     | 6448/14596 [1:01:45<1:08:20,  1.99it/s] 44%|████▍     | 6456/14596 [1:01:49<1:07:56,  2.00it/s] 44%|████▍     | 6464/14596 [1:01:53<1:08:14,  1.99it/s] 44%|████▍     | 6472/14596 [1:01:57<1:08:08,  1.99it/s] 44%|████▍     | 6480/14596 [1:02:01<1:07:55,  1.99it/s] 44%|████▍     | 6488/14596 [1:02:05<1:07:54,  1.99it/s] 45%|████▍     | 6496/14596 [1:02:09<1:07:39,  2.00it/s] 45%|████▍     | 6504/14596 [1:02:13<1:07:41,  1.99it/s] 45%|████▍     | 6512/14596 [1:02:17<1:07:30,  2.00it/s] 45%|████▍     | 6520/14596 [1:02:21<1:07:40,  1.99it/s] 45%|████▍     | 6528/14596 [1:02:25<1:07:30,  1.99it/s] 45%|████▍     | 6536/14596 [1:02:29<1:07:05,  2.00it/s] 45%|████▍     | 6544/14596 [1:02:33<1:07:04,  2.00it/s] 45%|████▍     | 6552/14596 [1:02:37<1:07:00,  2.00it/s] 45%|████▍     | 6560/14596 [1:02:41<1:06:59,  2.00it/s] 45%|████▍     | 6568/14596 [1:02:45<1:06:56,  2.00it/s] 45%|████▌     | 6576/14596 [1:02:49<1:06:47,  2.00it/s] 45%|████▌     | 6584/14596 [1:02:53<1:06:33,  2.01it/s] 45%|████▌     | 6592/14596 [1:02:57<1:06:40,  2.00it/s] 45%|████▌     | 6600/14596 [1:03:01<1:06:31,  2.00it/s] 45%|████▌     | 6608/14596 [1:03:05<1:06:20,  2.01it/s] 45%|████▌     | 6616/14596 [1:03:09<1:06:16,  2.01it/s] 45%|████▌     | 6624/14596 [1:03:13<1:05:58,  2.01it/s] 45%|████▌     | 6632/14596 [1:03:17<1:05:51,  2.02it/s] 45%|████▌     | 6640/14596 [1:03:21<1:05:55,  2.01it/s] 46%|████▌     | 6648/14596 [1:03:25<1:05:44,  2.01it/s] 46%|████▌     | 6656/14596 [1:03:29<1:05:37,  2.02it/s] 46%|████▌     | 6664/14596 [1:03:33<1:05:32,  2.02it/s] 46%|████▌     | 6672/14596 [1:03:37<1:05:48,  2.01it/s] 46%|████▌     | 6680/14596 [1:03:41<1:05:32,  2.01it/s] 46%|████▌     | 6688/14596 [1:03:44<1:05:27,  2.01it/s] 46%|████▌     | 6696/14596 [1:03:48<1:05:16,  2.02it/s] 46%|████▌     | 6704/14596 [1:03:52<1:05:23,  2.01it/s] 46%|████▌     | 6712/14596 [1:03:56<1:05:09,  2.02it/s] 46%|████▌     | 6720/14596 [1:04:00<1:04:50,  2.02it/s] 46%|████▌     | 6728/14596 [1:04:04<1:04:44,  2.03it/s] 46%|████▌     | 6736/14596 [1:04:08<1:04:25,  2.03it/s] 46%|████▌     | 6744/14596 [1:04:12<1:04:24,  2.03it/s] 46%|████▋     | 6752/14596 [1:04:16<1:04:11,  2.04it/s] 46%|████▋     | 6760/14596 [1:04:20<1:04:01,  2.04it/s] 46%|████▋     | 6768/14596 [1:04:24<1:03:57,  2.04it/s] 46%|████▋     | 6776/14596 [1:04:28<1:04:11,  2.03it/s] 46%|████▋     | 6784/14596 [1:04:32<1:03:46,  2.04it/s] 47%|████▋     | 6792/14596 [1:04:36<1:03:44,  2.04it/s] 47%|████▋     | 6800/14596 [1:04:40<1:03:38,  2.04it/s] 47%|████▋     | 6808/14596 [1:04:43<1:03:26,  2.05it/s] 47%|████▋     | 6816/14596 [1:04:47<1:03:14,  2.05it/s] 47%|████▋     | 6824/14596 [1:04:51<1:03:17,  2.05it/s] 47%|████▋     | 6832/14596 [1:04:55<1:03:23,  2.04it/s] 47%|████▋     | 6840/14596 [1:04:59<1:03:20,  2.04it/s] 47%|████▋     | 6848/14596 [1:05:03<1:03:09,  2.04it/s] 47%|████▋     | 6856/14596 [1:05:07<1:02:58,  2.05it/s] 47%|████▋     | 6864/14596 [1:05:11<1:02:53,  2.05it/s] 47%|████▋     | 6872/14596 [1:05:15<1:02:36,  2.06it/s] 47%|████▋     | 6880/14596 [1:05:19<1:02:43,  2.05it/s] 47%|████▋     | 6888/14596 [1:05:22<1:02:30,  2.06it/s] 47%|████▋     | 6896/14596 [1:05:26<1:02:35,  2.05it/s] 47%|████▋     | 6904/14596 [1:05:30<1:02:27,  2.05it/s] 47%|████▋     | 6912/14596 [1:05:34<1:02:22,  2.05it/s] 47%|████▋     | 6920/14596 [1:05:38<1:02:11,  2.06it/s] 47%|████▋     | 6928/14596 [1:05:42<1:02:20,  2.05it/s] 48%|████▊     | 6936/14596 [1:05:46<1:02:11,  2.05it/s] 48%|████▊     | 6944/14596 [1:05:50<1:02:00,  2.06it/s] 48%|████▊     | 6952/14596 [1:05:54<1:01:52,  2.06it/s] 48%|████▊     | 6960/14596 [1:05:57<1:01:43,  2.06it/s] 48%|████▊     | 6968/14596 [1:06:01<1:01:30,  2.07it/s] 48%|████▊     | 6976/14596 [1:06:05<1:01:28,  2.07it/s] 48%|████▊     | 6984/14596 [1:06:09<1:01:27,  2.06it/s] 48%|████▊     | 6992/14596 [1:06:13<1:01:20,  2.07it/s] 48%|████▊     | 7000/14596 [1:06:17<1:01:21,  2.06it/s] 48%|████▊     | 7008/14596 [1:06:21<1:01:09,  2.07it/s] 48%|████▊     | 7016/14596 [1:06:24<1:00:59,  2.07it/s] 48%|████▊     | 7024/14596 [1:06:28<1:00:44,  2.08it/s] 48%|████▊     | 7032/14596 [1:06:32<1:00:50,  2.07it/s] 48%|████▊     | 7040/14596 [1:06:36<1:01:01,  2.06it/s] 48%|████▊     | 7048/14596 [1:06:40<1:00:50,  2.07it/s] 48%|████▊     | 7056/14596 [1:06:44<1:00:29,  2.08it/s] 48%|████▊     | 7064/14596 [1:06:48<1:00:19,  2.08it/s] 48%|████▊     | 7072/14596 [1:06:51<1:00:19,  2.08it/s] 49%|████▊     | 7080/14596 [1:06:55<1:00:19,  2.08it/s] 49%|████▊     | 7088/14596 [1:06:59<59:59,  2.09it/s]   49%|████▊     | 7096/14596 [1:07:03<1:00:11,  2.08it/s] 49%|████▊     | 7104/14596 [1:07:07<1:00:01,  2.08it/s] 49%|████▊     | 7112/14596 [1:07:11<1:00:10,  2.07it/s] 49%|████▉     | 7120/14596 [1:07:15<1:00:04,  2.07it/s] 49%|████▉     | 7128/14596 [1:07:18<1:00:05,  2.07it/s] 49%|████▉     | 7136/14596 [1:07:22<59:47,  2.08it/s]   49%|████▉     | 7144/14596 [1:07:26<59:52,  2.07it/s] 49%|████▉     | 7152/14596 [1:07:30<59:42,  2.08it/s] 49%|████▉     | 7160/14596 [1:07:34<59:29,  2.08it/s] 49%|████▉     | 7168/14596 [1:07:38<59:27,  2.08it/s] 49%|████▉     | 7176/14596 [1:07:42<59:32,  2.08it/s] 49%|████▉     | 7184/14596 [1:07:45<59:21,  2.08it/s] 49%|████▉     | 7192/14596 [1:07:49<59:23,  2.08it/s] 49%|████▉     | 7200/14596 [1:07:53<59:09,  2.08it/s] 49%|████▉     | 7208/14596 [1:07:57<59:04,  2.08it/s] 49%|████▉     | 7216/14596 [1:08:01<59:00,  2.08it/s] 49%|████▉     | 7224/14596 [1:08:04<58:47,  2.09it/s] 50%|████▉     | 7232/14596 [1:08:08<58:47,  2.09it/s] 50%|████▉     | 7240/14596 [1:08:12<58:38,  2.09it/s] 50%|████▉     | 7248/14596 [1:08:16<58:35,  2.09it/s] 50%|████▉     | 7256/14596 [1:08:20<58:37,  2.09it/s] 50%|████▉     | 7264/14596 [1:08:24<58:30,  2.09it/s] 50%|████▉     | 7272/14596 [1:08:27<58:24,  2.09it/s] 50%|████▉     | 7280/14596 [1:08:31<58:20,  2.09it/s] 50%|████▉     | 7288/14596 [1:08:35<58:10,  2.09it/s] 50%|████▉     | 7296/14596 [1:08:39<58:08,  2.09it/s] 50%|█████     | 7304/14596 [1:08:43<57:52,  2.10it/s] 50%|█████     | 7312/14596 [1:08:47<57:57,  2.09it/s] 50%|█████     | 7320/14596 [1:08:50<57:40,  2.10it/s] 50%|█████     | 7328/14596 [1:08:54<57:27,  2.11it/s] 50%|█████     | 7336/14596 [1:08:58<57:29,  2.10it/s] 50%|█████     | 7344/14596 [1:09:02<57:20,  2.11it/s] 50%|█████     | 7352/14596 [1:09:06<57:27,  2.10it/s] 50%|█████     | 7360/14596 [1:09:09<57:13,  2.11it/s] 50%|█████     | 7368/14596 [1:09:13<57:08,  2.11it/s] 51%|█████     | 7376/14596 [1:09:17<56:59,  2.11it/s] 51%|█████     | 7384/14596 [1:09:21<56:56,  2.11it/s] 51%|█████     | 7392/14596 [1:09:24<56:38,  2.12it/s] 51%|█████     | 7400/14596 [1:09:28<56:37,  2.12it/s] 51%|█████     | 7408/14596 [1:09:32<56:19,  2.13it/s] 51%|█████     | 7416/14596 [1:09:36<56:18,  2.13it/s] 51%|█████     | 7424/14596 [1:09:39<56:17,  2.12it/s] 51%|█████     | 7432/14596 [1:09:43<56:19,  2.12it/s] 51%|█████     | 7440/14596 [1:09:47<55:54,  2.13it/s] 51%|█████     | 7448/14596 [1:09:51<56:02,  2.13it/s] 51%|█████     | 7456/14596 [1:09:54<55:51,  2.13it/s] 51%|█████     | 7464/14596 [1:09:58<55:45,  2.13it/s] 51%|█████     | 7472/14596 [1:10:02<55:53,  2.12it/s] 51%|█████     | 7480/14596 [1:10:06<55:34,  2.13it/s] 51%|█████▏    | 7488/14596 [1:10:10<55:49,  2.12it/s] 51%|█████▏    | 7496/14596 [1:10:13<55:40,  2.13it/s] 51%|█████▏    | 7504/14596 [1:10:17<55:28,  2.13it/s] 51%|█████▏    | 7512/14596 [1:10:21<55:42,  2.12it/s] 52%|█████▏    | 7520/14596 [1:10:25<55:29,  2.13it/s] 52%|█████▏    | 7528/14596 [1:10:28<55:28,  2.12it/s] 52%|█████▏    | 7536/14596 [1:10:32<55:24,  2.12it/s] 52%|█████▏    | 7544/14596 [1:10:36<55:14,  2.13it/s] 52%|█████▏    | 7552/14596 [1:10:40<55:00,  2.13it/s] 52%|█████▏    | 7560/14596 [1:10:43<55:08,  2.13it/s] 52%|█████▏    | 7568/14596 [1:10:47<54:50,  2.14it/s] 52%|█████▏    | 7576/14596 [1:10:51<54:46,  2.14it/s] 52%|█████▏    | 7584/14596 [1:10:55<54:49,  2.13it/s] 52%|█████▏    | 7592/14596 [1:10:58<54:28,  2.14it/s] 52%|█████▏    | 7600/14596 [1:11:02<54:32,  2.14it/s] 52%|█████▏    | 7608/14596 [1:11:06<54:18,  2.14it/s] 52%|█████▏    | 7616/14596 [1:11:10<54:18,  2.14it/s] 52%|█████▏    | 7624/14596 [1:11:13<54:31,  2.13it/s] 52%|█████▏    | 7632/14596 [1:11:17<54:29,  2.13it/s] 52%|█████▏    | 7640/14596 [1:11:21<54:07,  2.14it/s] 52%|█████▏    | 7648/14596 [1:11:24<53:52,  2.15it/s] 52%|█████▏    | 7656/14596 [1:11:28<53:55,  2.14it/s] 53%|█████▎    | 7664/14596 [1:11:32<53:51,  2.15it/s] 53%|█████▎    | 7672/14596 [1:11:36<53:41,  2.15it/s] 53%|█████▎    | 7680/14596 [1:11:39<53:43,  2.15it/s] 53%|█████▎    | 7688/14596 [1:11:43<53:35,  2.15it/s] 53%|█████▎    | 7696/14596 [1:11:47<53:26,  2.15it/s] 53%|█████▎    | 7704/14596 [1:11:50<53:14,  2.16it/s] 53%|█████▎    | 7712/14596 [1:11:54<53:09,  2.16it/s] 53%|█████▎    | 7720/14596 [1:11:58<53:00,  2.16it/s] 53%|█████▎    | 7728/14596 [1:12:02<52:49,  2.17it/s] 53%|█████▎    | 7736/14596 [1:12:05<52:39,  2.17it/s] 53%|█████▎    | 7744/14596 [1:12:09<52:48,  2.16it/s] 53%|█████▎    | 7752/14596 [1:12:13<52:31,  2.17it/s] 53%|█████▎    | 7760/14596 [1:12:16<52:22,  2.18it/s] 53%|█████▎    | 7768/14596 [1:12:20<52:30,  2.17it/s] 53%|█████▎    | 7776/14596 [1:12:24<52:22,  2.17it/s] 53%|█████▎    | 7784/14596 [1:12:27<52:11,  2.18it/s] 53%|█████▎    | 7792/14596 [1:12:31<52:04,  2.18it/s] 53%|█████▎    | 7800/14596 [1:12:35<52:00,  2.18it/s] 53%|█████▎    | 7808/14596 [1:12:38<52:03,  2.17it/s] 54%|█████▎    | 7816/14596 [1:12:42<51:47,  2.18it/s] 54%|█████▎    | 7824/14596 [1:12:46<51:39,  2.19it/s] 54%|█████▎    | 7832/14596 [1:12:49<51:31,  2.19it/s] 54%|█████▎    | 7840/14596 [1:12:53<51:24,  2.19it/s] 54%|█████▍    | 7848/14596 [1:12:57<51:11,  2.20it/s] 54%|█████▍    | 7856/14596 [1:13:00<51:10,  2.19it/s] 54%|█████▍    | 7864/14596 [1:13:04<51:10,  2.19it/s] 54%|█████▍    | 7872/14596 [1:13:07<51:03,  2.19it/s] 54%|█████▍    | 7880/14596 [1:13:11<50:58,  2.20it/s] 54%|█████▍    | 7888/14596 [1:13:15<51:03,  2.19it/s] 54%|█████▍    | 7896/14596 [1:13:18<50:59,  2.19it/s] 54%|█████▍    | 7904/14596 [1:13:22<50:50,  2.19it/s] 54%|█████▍    | 7912/14596 [1:13:26<50:50,  2.19it/s] 54%|█████▍    | 7920/14596 [1:13:29<50:42,  2.19it/s] 54%|█████▍    | 7928/14596 [1:13:33<50:34,  2.20it/s] 54%|█████▍    | 7936/14596 [1:13:37<50:32,  2.20it/s] 54%|█████▍    | 7944/14596 [1:13:40<50:37,  2.19it/s] 54%|█████▍    | 7952/14596 [1:13:44<50:41,  2.18it/s] 55%|█████▍    | 7960/14596 [1:13:48<50:29,  2.19it/s] 55%|█████▍    | 7968/14596 [1:13:51<50:21,  2.19it/s] 55%|█████▍    | 7976/14596 [1:13:55<50:12,  2.20it/s] 55%|█████▍    | 7984/14596 [1:13:58<50:00,  2.20it/s] 55%|█████▍    | 7992/14596 [1:14:02<50:05,  2.20it/s] 55%|█████▍    | 8000/14596 [1:14:06<49:56,  2.20it/s] 55%|█████▍    | 8008/14596 [1:14:09<49:59,  2.20it/s] 55%|█████▍    | 8016/14596 [1:14:13<49:44,  2.20it/s] 55%|█████▍    | 8024/14596 [1:14:17<49:37,  2.21it/s] 55%|█████▌    | 8032/14596 [1:14:20<49:35,  2.21it/s] 55%|█████▌    | 8040/14596 [1:14:24<49:33,  2.20it/s] 55%|█████▌    | 8048/14596 [1:14:28<49:26,  2.21it/s] 55%|█████▌    | 8056/14596 [1:14:31<49:25,  2.21it/s] 55%|█████▌    | 8064/14596 [1:14:35<49:18,  2.21it/s] 55%|█████▌    | 8072/14596 [1:14:38<49:20,  2.20it/s] 55%|█████▌    | 8080/14596 [1:14:42<49:05,  2.21it/s] 55%|█████▌    | 8088/14596 [1:14:46<49:09,  2.21it/s] 55%|█████▌    | 8096/14596 [1:14:49<49:10,  2.20it/s] 56%|█████▌    | 8104/14596 [1:14:53<49:06,  2.20it/s] 56%|█████▌    | 8112/14596 [1:14:57<49:02,  2.20it/s] 56%|█████▌    | 8120/14596 [1:15:00<48:53,  2.21it/s] 56%|█████▌    | 8128/14596 [1:15:04<48:46,  2.21it/s] 56%|█████▌    | 8136/14596 [1:15:07<48:45,  2.21it/s] 56%|█████▌    | 8144/14596 [1:15:11<48:32,  2.21it/s] 56%|█████▌    | 8152/14596 [1:15:15<48:31,  2.21it/s] 56%|█████▌    | 8160/14596 [1:15:18<48:15,  2.22it/s] 56%|█████▌    | 8168/14596 [1:15:22<48:00,  2.23it/s] 56%|█████▌    | 8176/14596 [1:15:25<48:04,  2.23it/s] 56%|█████▌    | 8184/14596 [1:15:29<48:16,  2.21it/s] 56%|█████▌    | 8192/14596 [1:15:33<48:06,  2.22it/s] 56%|█████▌    | 8200/14596 [1:15:36<48:05,  2.22it/s] 56%|█████▌    | 8208/14596 [1:15:40<47:48,  2.23it/s] 56%|█████▋    | 8216/14596 [1:15:43<47:47,  2.22it/s] 56%|█████▋    | 8224/14596 [1:15:47<47:26,  2.24it/s] 56%|█████▋    | 8232/14596 [1:15:50<47:31,  2.23it/s] 56%|█████▋    | 8240/14596 [1:15:54<47:14,  2.24it/s] 57%|█████▋    | 8248/14596 [1:15:58<47:16,  2.24it/s] 57%|█████▋    | 8256/14596 [1:16:01<47:09,  2.24it/s] 57%|█████▋    | 8264/14596 [1:16:05<47:13,  2.23it/s] 57%|█████▋    | 8272/14596 [1:16:08<46:55,  2.25it/s] 57%|█████▋    | 8280/14596 [1:16:12<47:02,  2.24it/s] 57%|█████▋    | 8288/14596 [1:16:15<46:45,  2.25it/s] 57%|█████▋    | 8296/14596 [1:16:19<46:38,  2.25it/s] 57%|█████▋    | 8304/14596 [1:16:22<46:27,  2.26it/s] 57%|█████▋    | 8312/14596 [1:16:26<46:20,  2.26it/s] 57%|█████▋    | 8320/14596 [1:16:30<46:15,  2.26it/s] 57%|█████▋    | 8328/14596 [1:16:33<46:22,  2.25it/s] 57%|█████▋    | 8336/14596 [1:16:37<46:21,  2.25it/s] 57%|█████▋    | 8344/14596 [1:16:40<46:12,  2.26it/s] 57%|█████▋    | 8352/14596 [1:16:44<45:59,  2.26it/s] 57%|█████▋    | 8360/14596 [1:16:47<46:03,  2.26it/s] 57%|█████▋    | 8368/14596 [1:16:51<45:56,  2.26it/s] 57%|█████▋    | 8376/14596 [1:16:54<46:00,  2.25it/s] 57%|█████▋    | 8384/14596 [1:16:58<45:54,  2.25it/s] 57%|█████▋    | 8392/14596 [1:17:01<45:41,  2.26it/s] 58%|█████▊    | 8400/14596 [1:17:05<45:43,  2.26it/s] 58%|█████▊    | 8408/14596 [1:17:09<45:38,  2.26it/s] 58%|█████▊    | 8416/14596 [1:17:12<45:31,  2.26it/s] 58%|█████▊    | 8424/14596 [1:17:16<45:18,  2.27it/s] 58%|█████▊    | 8432/14596 [1:17:19<45:14,  2.27it/s] 58%|█████▊    | 8440/14596 [1:17:23<45:05,  2.28it/s] 58%|█████▊    | 8448/14596 [1:17:26<45:02,  2.27it/s] 58%|█████▊    | 8456/14596 [1:17:30<44:56,  2.28it/s] 58%|█████▊    | 8464/14596 [1:17:33<44:53,  2.28it/s] 58%|█████▊    | 8472/14596 [1:17:37<45:05,  2.26it/s] 58%|█████▊    | 8480/14596 [1:17:40<44:49,  2.27it/s] 58%|█████▊    | 8488/14596 [1:17:44<44:57,  2.26it/s] 58%|█████▊    | 8496/14596 [1:17:47<44:48,  2.27it/s] 58%|█████▊    | 8504/14596 [1:17:51<44:32,  2.28it/s] 58%|█████▊    | 8512/14596 [1:17:54<44:25,  2.28it/s] 58%|█████▊    | 8520/14596 [1:17:58<44:08,  2.29it/s] 58%|█████▊    | 8528/14596 [1:18:01<44:17,  2.28it/s] 58%|█████▊    | 8536/14596 [1:18:05<44:12,  2.28it/s] 59%|█████▊    | 8544/14596 [1:18:08<44:09,  2.28it/s] 59%|█████▊    | 8552/14596 [1:18:12<44:03,  2.29it/s] 59%|█████▊    | 8560/14596 [1:18:15<44:03,  2.28it/s] 59%|█████▊    | 8568/14596 [1:18:19<43:43,  2.30it/s] 59%|█████▉    | 8576/14596 [1:18:22<43:43,  2.29it/s] 59%|█████▉    | 8584/14596 [1:18:26<43:31,  2.30it/s] 59%|█████▉    | 8592/14596 [1:18:29<43:23,  2.31it/s] 59%|█████▉    | 8600/14596 [1:18:32<43:09,  2.32it/s] 59%|█████▉    | 8608/14596 [1:18:36<43:05,  2.32it/s] 59%|█████▉    | 8616/14596 [1:18:39<43:14,  2.30it/s] 59%|█████▉    | 8624/14596 [1:18:43<43:13,  2.30it/s] 59%|█████▉    | 8632/14596 [1:18:46<43:10,  2.30it/s] 59%|█████▉    | 8640/14596 [1:18:50<43:15,  2.29it/s] 59%|█████▉    | 8648/14596 [1:18:53<43:05,  2.30it/s] 59%|█████▉    | 8656/14596 [1:18:57<43:07,  2.30it/s] 59%|█████▉    | 8664/14596 [1:19:00<42:52,  2.31it/s] 59%|█████▉    | 8672/14596 [1:19:04<42:47,  2.31it/s] 59%|█████▉    | 8680/14596 [1:19:07<42:51,  2.30it/s] 60%|█████▉    | 8688/14596 [1:19:11<42:30,  2.32it/s] 60%|█████▉    | 8696/14596 [1:19:14<42:26,  2.32it/s] 60%|█████▉    | 8704/14596 [1:19:18<42:17,  2.32it/s] 60%|█████▉    | 8712/14596 [1:19:21<42:09,  2.33it/s] 60%|█████▉    | 8720/14596 [1:19:24<42:03,  2.33it/s] 60%|█████▉    | 8728/14596 [1:19:28<42:08,  2.32it/s] 60%|█████▉    | 8736/14596 [1:19:31<41:57,  2.33it/s] 60%|█████▉    | 8744/14596 [1:19:35<41:49,  2.33it/s] 60%|█████▉    | 8752/14596 [1:19:38<41:43,  2.33it/s] 60%|██████    | 8760/14596 [1:19:42<41:44,  2.33it/s] 60%|██████    | 8768/14596 [1:19:45<41:44,  2.33it/s] 60%|██████    | 8776/14596 [1:19:48<41:41,  2.33it/s] 60%|██████    | 8784/14596 [1:19:52<41:35,  2.33it/s] 60%|██████    | 8792/14596 [1:19:55<41:34,  2.33it/s] 60%|██████    | 8800/14596 [1:19:59<41:33,  2.32it/s] 60%|██████    | 8808/14596 [1:20:02<41:47,  2.31it/s] 60%|██████    | 8816/14596 [1:20:06<41:31,  2.32it/s] 60%|██████    | 8824/14596 [1:20:09<41:17,  2.33it/s] 61%|██████    | 8832/14596 [1:20:13<41:04,  2.34it/s] 61%|██████    | 8840/14596 [1:20:16<41:00,  2.34it/s] 61%|██████    | 8848/14596 [1:20:19<41:15,  2.32it/s] 61%|██████    | 8856/14596 [1:20:23<41:04,  2.33it/s] 61%|██████    | 8864/14596 [1:20:26<41:02,  2.33it/s] 61%|██████    | 8872/14596 [1:20:30<41:05,  2.32it/s] 61%|██████    | 8880/14596 [1:20:33<41:00,  2.32it/s] 61%|██████    | 8888/14596 [1:20:37<40:51,  2.33it/s] 61%|██████    | 8896/14596 [1:20:40<40:38,  2.34it/s] 61%|██████    | 8904/14596 [1:20:43<40:36,  2.34it/s] 61%|██████    | 8912/14596 [1:20:47<40:24,  2.34it/s] 61%|██████    | 8920/14596 [1:20:50<40:33,  2.33it/s] 61%|██████    | 8928/14596 [1:20:54<40:29,  2.33it/s] 61%|██████    | 8936/14596 [1:20:57<40:19,  2.34it/s] 61%|██████▏   | 8944/14596 [1:21:01<40:12,  2.34it/s] 61%|██████▏   | 8952/14596 [1:21:04<40:09,  2.34it/s] 61%|██████▏   | 8960/14596 [1:21:07<40:03,  2.35it/s] 61%|██████▏   | 8968/14596 [1:21:11<39:45,  2.36it/s] 61%|██████▏   | 8976/14596 [1:21:14<39:44,  2.36it/s] 62%|██████▏   | 8984/14596 [1:21:17<39:40,  2.36it/s] 62%|██████▏   | 8992/14596 [1:21:21<39:32,  2.36it/s] 62%|██████▏   | 9000/14596 [1:21:24<39:20,  2.37it/s] 62%|██████▏   | 9008/14596 [1:21:28<39:09,  2.38it/s] 62%|██████▏   | 9016/14596 [1:21:31<39:11,  2.37it/s] 62%|██████▏   | 9024/14596 [1:21:34<39:14,  2.37it/s] 62%|██████▏   | 9032/14596 [1:21:38<39:03,  2.37it/s] 62%|██████▏   | 9040/14596 [1:21:41<38:55,  2.38it/s] 62%|██████▏   | 9048/14596 [1:21:44<38:52,  2.38it/s] 62%|██████▏   | 9056/14596 [1:21:48<38:46,  2.38it/s] 62%|██████▏   | 9064/14596 [1:21:51<38:50,  2.37it/s] 62%|██████▏   | 9072/14596 [1:21:54<38:28,  2.39it/s] 62%|██████▏   | 9080/14596 [1:21:58<38:42,  2.38it/s] 62%|██████▏   | 9088/14596 [1:22:01<38:26,  2.39it/s] 62%|██████▏   | 9096/14596 [1:22:05<38:29,  2.38it/s] 62%|██████▏   | 9104/14596 [1:22:08<38:23,  2.38it/s] 62%|██████▏   | 9112/14596 [1:22:11<38:19,  2.38it/s] 62%|██████▏   | 9120/14596 [1:22:15<38:21,  2.38it/s] 63%|██████▎   | 9128/14596 [1:22:18<38:17,  2.38it/s] 63%|██████▎   | 9136/14596 [1:22:21<38:07,  2.39it/s] 63%|██████▎   | 9144/14596 [1:22:25<38:04,  2.39it/s] 63%|██████▎   | 9152/14596 [1:22:28<37:57,  2.39it/s] 63%|██████▎   | 9160/14596 [1:22:31<37:52,  2.39it/s] 63%|██████▎   | 9168/14596 [1:22:35<37:52,  2.39it/s] 63%|██████▎   | 9176/14596 [1:22:38<37:51,  2.39it/s] 63%|██████▎   | 9184/14596 [1:22:41<37:49,  2.38it/s] 63%|██████▎   | 9192/14596 [1:22:45<37:48,  2.38it/s] 63%|██████▎   | 9200/14596 [1:22:48<37:43,  2.38it/s] 63%|██████▎   | 9208/14596 [1:22:51<37:39,  2.38it/s] 63%|██████▎   | 9216/14596 [1:22:55<37:29,  2.39it/s] 63%|██████▎   | 9224/14596 [1:22:58<37:25,  2.39it/s] 63%|██████▎   | 9232/14596 [1:23:02<37:31,  2.38it/s] 63%|██████▎   | 9240/14596 [1:23:05<37:29,  2.38it/s] 63%|██████▎   | 9248/14596 [1:23:08<37:21,  2.39it/s] 63%|██████▎   | 9256/14596 [1:23:12<37:11,  2.39it/s] 63%|██████▎   | 9264/14596 [1:23:15<37:10,  2.39it/s] 64%|██████▎   | 9272/14596 [1:23:18<37:11,  2.39it/s] 64%|██████▎   | 9280/14596 [1:23:22<37:09,  2.38it/s] 64%|██████▎   | 9288/14596 [1:23:25<37:00,  2.39it/s] 64%|██████▎   | 9296/14596 [1:23:28<36:55,  2.39it/s] 64%|██████▎   | 9304/14596 [1:23:32<36:50,  2.39it/s] 64%|██████▍   | 9312/14596 [1:23:35<36:43,  2.40it/s] 64%|██████▍   | 9320/14596 [1:23:38<36:53,  2.38it/s] 64%|██████▍   | 9328/14596 [1:23:42<36:33,  2.40it/s] 64%|██████▍   | 9336/14596 [1:23:45<36:32,  2.40it/s] 64%|██████▍   | 9344/14596 [1:23:48<36:23,  2.41it/s] 64%|██████▍   | 9352/14596 [1:23:52<36:20,  2.41it/s] 64%|██████▍   | 9360/14596 [1:23:55<36:12,  2.41it/s] 64%|██████▍   | 9368/14596 [1:23:58<36:21,  2.40it/s] 64%|██████▍   | 9376/14596 [1:24:02<36:09,  2.41it/s] 64%|██████▍   | 9384/14596 [1:24:05<36:03,  2.41it/s] 64%|██████▍   | 9392/14596 [1:24:08<36:07,  2.40it/s] 64%|██████▍   | 9400/14596 [1:24:12<36:01,  2.40it/s] 64%|██████▍   | 9408/14596 [1:24:15<35:59,  2.40it/s] 65%|██████▍   | 9416/14596 [1:24:18<35:46,  2.41it/s] 65%|██████▍   | 9424/14596 [1:24:21<35:44,  2.41it/s] 65%|██████▍   | 9432/14596 [1:24:25<35:48,  2.40it/s] 65%|██████▍   | 9440/14596 [1:24:28<35:31,  2.42it/s] 65%|██████▍   | 9448/14596 [1:24:31<35:30,  2.42it/s] 65%|██████▍   | 9456/14596 [1:24:35<35:31,  2.41it/s] 65%|██████▍   | 9464/14596 [1:24:38<35:27,  2.41it/s] 65%|██████▍   | 9472/14596 [1:24:41<35:17,  2.42it/s] 65%|██████▍   | 9480/14596 [1:24:45<35:19,  2.41it/s] 65%|██████▌   | 9488/14596 [1:24:48<35:07,  2.42it/s] 65%|██████▌   | 9496/14596 [1:24:51<35:05,  2.42it/s] 65%|██████▌   | 9504/14596 [1:24:55<35:03,  2.42it/s] 65%|██████▌   | 9512/14596 [1:24:58<34:57,  2.42it/s] 65%|██████▌   | 9520/14596 [1:25:01<34:57,  2.42it/s] 65%|██████▌   | 9528/14596 [1:25:04<34:50,  2.42it/s] 65%|██████▌   | 9536/14596 [1:25:08<34:48,  2.42it/s] 65%|██████▌   | 9544/14596 [1:25:11<34:34,  2.44it/s] 65%|██████▌   | 9552/14596 [1:25:14<34:36,  2.43it/s] 65%|██████▌   | 9560/14596 [1:25:18<34:20,  2.44it/s] 66%|██████▌   | 9568/14596 [1:25:21<34:26,  2.43it/s] 66%|██████▌   | 9576/14596 [1:25:24<34:18,  2.44it/s] 66%|██████▌   | 9584/14596 [1:25:27<34:22,  2.43it/s] 66%|██████▌   | 9592/14596 [1:25:31<34:08,  2.44it/s] 66%|██████▌   | 9600/14596 [1:25:34<34:18,  2.43it/s] 66%|██████▌   | 9608/14596 [1:25:37<34:13,  2.43it/s] 66%|██████▌   | 9616/14596 [1:25:41<34:10,  2.43it/s] 66%|██████▌   | 9624/14596 [1:25:44<34:00,  2.44it/s] 66%|██████▌   | 9632/14596 [1:25:47<33:47,  2.45it/s] 66%|██████▌   | 9640/14596 [1:25:50<33:48,  2.44it/s] 66%|██████▌   | 9648/14596 [1:25:54<33:48,  2.44it/s] 66%|██████▌   | 9656/14596 [1:25:57<33:43,  2.44it/s] 66%|██████▌   | 9664/14596 [1:26:00<33:36,  2.45it/s] 66%|██████▋   | 9672/14596 [1:26:03<33:31,  2.45it/s] 66%|██████▋   | 9680/14596 [1:26:07<33:25,  2.45it/s] 66%|██████▋   | 9688/14596 [1:26:10<33:29,  2.44it/s] 66%|██████▋   | 9696/14596 [1:26:13<33:29,  2.44it/s] 66%|██████▋   | 9704/14596 [1:26:17<33:25,  2.44it/s] 67%|██████▋   | 9712/14596 [1:26:20<33:20,  2.44it/s] 67%|██████▋   | 9720/14596 [1:26:23<33:18,  2.44it/s] 67%|██████▋   | 9728/14596 [1:26:26<33:15,  2.44it/s] 67%|██████▋   | 9736/14596 [1:26:30<33:03,  2.45it/s] 67%|██████▋   | 9744/14596 [1:26:33<32:57,  2.45it/s] 67%|██████▋   | 9752/14596 [1:26:36<32:53,  2.45it/s] 67%|██████▋   | 9760/14596 [1:26:39<32:42,  2.46it/s] 67%|██████▋   | 9768/14596 [1:26:43<32:44,  2.46it/s] 67%|██████▋   | 9776/14596 [1:26:46<32:41,  2.46it/s] 67%|██████▋   | 9784/14596 [1:26:49<32:37,  2.46it/s] 67%|██████▋   | 9792/14596 [1:26:52<32:34,  2.46it/s] 67%|██████▋   | 9800/14596 [1:26:56<32:29,  2.46it/s] 67%|██████▋   | 9808/14596 [1:26:59<32:30,  2.45it/s] 67%|██████▋   | 9816/14596 [1:27:02<32:27,  2.45it/s] 67%|██████▋   | 9824/14596 [1:27:05<32:21,  2.46it/s] 67%|██████▋   | 9832/14596 [1:27:09<32:22,  2.45it/s] 67%|██████▋   | 9840/14596 [1:27:12<32:11,  2.46it/s] 67%|██████▋   | 9848/14596 [1:27:15<32:12,  2.46it/s] 68%|██████▊   | 9856/14596 [1:27:18<32:08,  2.46it/s] 68%|██████▊   | 9864/14596 [1:27:22<31:59,  2.47it/s] 68%|██████▊   | 9872/14596 [1:27:25<31:52,  2.47it/s] 68%|██████▊   | 9880/14596 [1:27:28<31:52,  2.47it/s] 68%|██████▊   | 9888/14596 [1:27:31<31:51,  2.46it/s] 68%|██████▊   | 9896/14596 [1:27:35<31:45,  2.47it/s] 68%|██████▊   | 9904/14596 [1:27:38<31:42,  2.47it/s] 68%|██████▊   | 9912/14596 [1:27:41<31:42,  2.46it/s] 68%|██████▊   | 9920/14596 [1:27:44<31:38,  2.46it/s] 68%|██████▊   | 9928/14596 [1:27:48<31:36,  2.46it/s] 68%|██████▊   | 9936/14596 [1:27:51<31:29,  2.47it/s] 68%|██████▊   | 9944/14596 [1:27:54<31:19,  2.47it/s] 68%|██████▊   | 9952/14596 [1:27:57<31:16,  2.47it/s] 68%|██████▊   | 9960/14596 [1:28:01<31:11,  2.48it/s] 68%|██████▊   | 9968/14596 [1:28:04<31:08,  2.48it/s] 68%|██████▊   | 9976/14596 [1:28:07<31:02,  2.48it/s] 68%|██████▊   | 9984/14596 [1:28:10<30:54,  2.49it/s] 68%|██████▊   | 9992/14596 [1:28:13<30:56,  2.48it/s] 69%|██████▊   | 10000/14596 [1:28:17<30:53,  2.48it/s] 69%|██████▊   | 10008/14596 [1:28:20<30:51,  2.48it/s] 69%|██████▊   | 10016/14596 [1:28:23<30:46,  2.48it/s] 69%|██████▊   | 10024/14596 [1:28:26<30:47,  2.48it/s] 69%|██████▊   | 10032/14596 [1:28:30<30:40,  2.48it/s] 69%|██████▉   | 10040/14596 [1:28:33<30:39,  2.48it/s] 69%|██████▉   | 10048/14596 [1:28:36<30:35,  2.48it/s] 69%|██████▉   | 10056/14596 [1:28:39<30:30,  2.48it/s] 69%|██████▉   | 10064/14596 [1:28:42<30:24,  2.48it/s] 69%|██████▉   | 10072/14596 [1:28:46<30:20,  2.48it/s] 69%|██████▉   | 10080/14596 [1:28:49<30:12,  2.49it/s] 69%|██████▉   | 10088/14596 [1:28:52<30:12,  2.49it/s] 69%|██████▉   | 10096/14596 [1:28:55<30:01,  2.50it/s] 69%|██████▉   | 10104/14596 [1:28:59<30:07,  2.48it/s] 69%|██████▉   | 10112/14596 [1:29:02<30:02,  2.49it/s] 69%|██████▉   | 10120/14596 [1:29:05<29:55,  2.49it/s] 69%|██████▉   | 10128/14596 [1:29:08<29:51,  2.49it/s] 69%|██████▉   | 10136/14596 [1:29:11<29:51,  2.49it/s] 69%|██████▉   | 10144/14596 [1:29:15<29:46,  2.49it/s] 70%|██████▉   | 10152/14596 [1:29:18<29:38,  2.50it/s] 70%|██████▉   | 10160/14596 [1:29:21<29:28,  2.51it/s] 70%|██████▉   | 10168/14596 [1:29:24<29:27,  2.51it/s] 70%|██████▉   | 10176/14596 [1:29:27<29:21,  2.51it/s] 70%|██████▉   | 10184/14596 [1:29:30<29:12,  2.52it/s] 70%|██████▉   | 10192/14596 [1:29:34<29:15,  2.51it/s] 70%|██████▉   | 10200/14596 [1:29:37<29:12,  2.51it/s] 70%|██████▉   | 10208/14596 [1:29:40<29:11,  2.51it/s] 70%|██████▉   | 10216/14596 [1:29:43<29:00,  2.52it/s] 70%|███████   | 10224/14596 [1:29:46<29:02,  2.51it/s] 70%|███████   | 10232/14596 [1:29:50<28:56,  2.51it/s] 70%|███████   | 10240/14596 [1:29:53<28:47,  2.52it/s] 70%|███████   | 10248/14596 [1:29:56<28:49,  2.51it/s] 70%|███████   | 10256/14596 [1:29:59<28:41,  2.52it/s] 70%|███████   | 10264/14596 [1:30:02<28:35,  2.53it/s] 70%|███████   | 10272/14596 [1:30:05<28:32,  2.52it/s] 70%|███████   | 10280/14596 [1:30:09<28:32,  2.52it/s] 70%|███████   | 10288/14596 [1:30:12<28:27,  2.52it/s] 71%|███████   | 10296/14596 [1:30:15<28:22,  2.53it/s] 71%|███████   | 10304/14596 [1:30:18<28:20,  2.52it/s] 71%|███████   | 10312/14596 [1:30:21<28:18,  2.52it/s] 71%|███████   | 10320/14596 [1:30:24<28:09,  2.53it/s] 71%|███████   | 10328/14596 [1:30:28<28:04,  2.53it/s] 71%|███████   | 10336/14596 [1:30:31<28:10,  2.52it/s] 71%|███████   | 10344/14596 [1:30:34<28:03,  2.53it/s] 71%|███████   | 10352/14596 [1:30:37<28:01,  2.52it/s] 71%|███████   | 10360/14596 [1:30:40<27:59,  2.52it/s] 71%|███████   | 10368/14596 [1:30:43<27:55,  2.52it/s] 71%|███████   | 10376/14596 [1:30:47<27:45,  2.53it/s] 71%|███████   | 10384/14596 [1:30:50<27:42,  2.53it/s] 71%|███████   | 10392/14596 [1:30:53<27:36,  2.54it/s] 71%|███████▏  | 10400/14596 [1:30:56<27:38,  2.53it/s] 71%|███████▏  | 10408/14596 [1:30:59<27:32,  2.53it/s] 71%|███████▏  | 10416/14596 [1:31:02<27:31,  2.53it/s] 71%|███████▏  | 10424/14596 [1:31:06<27:22,  2.54it/s] 71%|███████▏  | 10432/14596 [1:31:09<27:27,  2.53it/s] 72%|███████▏  | 10440/14596 [1:31:12<27:20,  2.53it/s] 72%|███████▏  | 10448/14596 [1:31:15<27:14,  2.54it/s] 72%|███████▏  | 10456/14596 [1:31:18<27:06,  2.55it/s] 72%|███████▏  | 10464/14596 [1:31:21<27:06,  2.54it/s] 72%|███████▏  | 10472/14596 [1:31:24<27:05,  2.54it/s] 72%|███████▏  | 10480/14596 [1:31:28<26:59,  2.54it/s] 72%|███████▏  | 10488/14596 [1:31:31<26:54,  2.54it/s] 72%|███████▏  | 10496/14596 [1:31:34<26:52,  2.54it/s] 72%|███████▏  | 10504/14596 [1:31:37<26:49,  2.54it/s] 72%|███████▏  | 10512/14596 [1:31:40<26:51,  2.53it/s] 72%|███████▏  | 10520/14596 [1:31:43<26:44,  2.54it/s] 72%|███████▏  | 10528/14596 [1:31:46<26:43,  2.54it/s] 72%|███████▏  | 10536/14596 [1:31:50<26:33,  2.55it/s] 72%|███████▏  | 10544/14596 [1:31:53<26:30,  2.55it/s] 72%|███████▏  | 10552/14596 [1:31:56<26:25,  2.55it/s] 72%|███████▏  | 10560/14596 [1:31:59<26:22,  2.55it/s] 72%|███████▏  | 10568/14596 [1:32:02<26:15,  2.56it/s] 72%|███████▏  | 10576/14596 [1:32:05<26:18,  2.55it/s] 73%|███████▎  | 10584/14596 [1:32:08<26:13,  2.55it/s] 73%|███████▎  | 10592/14596 [1:32:12<26:08,  2.55it/s] 73%|███████▎  | 10600/14596 [1:32:15<26:04,  2.55it/s] 73%|███████▎  | 10608/14596 [1:32:18<26:06,  2.55it/s] 73%|███████▎  | 10616/14596 [1:32:21<25:58,  2.55it/s] 73%|███████▎  | 10624/14596 [1:32:24<25:55,  2.55it/s] 73%|███████▎  | 10632/14596 [1:32:27<25:49,  2.56it/s] 73%|███████▎  | 10640/14596 [1:32:30<25:51,  2.55it/s] 73%|███████▎  | 10648/14596 [1:32:33<25:43,  2.56it/s] 73%|███████▎  | 10656/14596 [1:32:37<25:42,  2.55it/s] 73%|███████▎  | 10664/14596 [1:32:40<25:29,  2.57it/s] 73%|███████▎  | 10672/14596 [1:32:43<25:33,  2.56it/s] 73%|███████▎  | 10680/14596 [1:32:46<25:31,  2.56it/s] 73%|███████▎  | 10688/14596 [1:32:49<25:25,  2.56it/s] 73%|███████▎  | 10696/14596 [1:32:52<25:22,  2.56it/s] 73%|███████▎  | 10704/14596 [1:32:55<25:17,  2.56it/s] 73%|███████▎  | 10712/14596 [1:32:58<25:05,  2.58it/s] 73%|███████▎  | 10720/14596 [1:33:01<25:07,  2.57it/s] 73%|███████▎  | 10728/14596 [1:33:05<25:00,  2.58it/s] 74%|███████▎  | 10736/14596 [1:33:08<24:58,  2.58it/s] 74%|███████▎  | 10744/14596 [1:33:11<24:58,  2.57it/s] 74%|███████▎  | 10752/14596 [1:33:14<24:49,  2.58it/s] 74%|███████▎  | 10760/14596 [1:33:17<24:45,  2.58it/s] 74%|███████▍  | 10768/14596 [1:33:20<24:47,  2.57it/s] 74%|███████▍  | 10776/14596 [1:33:23<24:38,  2.58it/s] 74%|███████▍  | 10784/14596 [1:33:26<24:38,  2.58it/s] 74%|███████▍  | 10792/14596 [1:33:29<24:32,  2.58it/s] 74%|███████▍  | 10800/14596 [1:33:32<24:27,  2.59it/s] 74%|███████▍  | 10808/14596 [1:33:36<24:20,  2.59it/s] 74%|███████▍  | 10816/14596 [1:33:39<24:15,  2.60it/s] 74%|███████▍  | 10824/14596 [1:33:42<24:12,  2.60it/s] 74%|███████▍  | 10832/14596 [1:33:45<24:13,  2.59it/s] 74%|███████▍  | 10840/14596 [1:33:48<24:13,  2.58it/s] 74%|███████▍  | 10848/14596 [1:33:51<24:02,  2.60it/s] 74%|███████▍  | 10856/14596 [1:33:54<23:57,  2.60it/s] 74%|███████▍  | 10864/14596 [1:33:57<23:51,  2.61it/s] 74%|███████▍  | 10872/14596 [1:34:00<23:52,  2.60it/s] 75%|███████▍  | 10880/14596 [1:34:03<23:58,  2.58it/s] 75%|███████▍  | 10888/14596 [1:34:06<23:50,  2.59it/s] 75%|███████▍  | 10896/14596 [1:34:09<23:48,  2.59it/s] 75%|███████▍  | 10904/14596 [1:34:13<23:44,  2.59it/s] 75%|███████▍  | 10912/14596 [1:34:16<23:37,  2.60it/s] 75%|███████▍  | 10920/14596 [1:34:19<23:42,  2.58it/s] 75%|███████▍  | 10928/14596 [1:34:22<23:37,  2.59it/s] 75%|███████▍  | 10936/14596 [1:34:25<23:39,  2.58it/s] 75%|███████▍  | 10944/14596 [1:34:28<23:30,  2.59it/s] 75%|███████▌  | 10952/14596 [1:34:31<23:31,  2.58it/s] 75%|███████▌  | 10960/14596 [1:34:34<23:26,  2.58it/s] 75%|███████▌  | 10968/14596 [1:34:37<23:21,  2.59it/s] 75%|███████▌  | 10976/14596 [1:34:40<23:21,  2.58it/s] 75%|███████▌  | 10984/14596 [1:34:43<23:14,  2.59it/s] 75%|███████▌  | 10992/14596 [1:34:46<23:04,  2.60it/s] 75%|███████▌  | 11000/14596 [1:34:50<23:05,  2.59it/s] 75%|███████▌  | 11008/14596 [1:34:53<23:02,  2.60it/s] 75%|███████▌  | 11016/14596 [1:34:56<22:57,  2.60it/s] 76%|███████▌  | 11024/14596 [1:34:59<22:47,  2.61it/s] 76%|███████▌  | 11032/14596 [1:35:02<22:55,  2.59it/s] 76%|███████▌  | 11040/14596 [1:35:05<22:44,  2.61it/s] 76%|███████▌  | 11048/14596 [1:35:08<22:51,  2.59it/s] 76%|███████▌  | 11056/14596 [1:35:11<22:44,  2.59it/s] 76%|███████▌  | 11064/14596 [1:35:14<22:41,  2.59it/s] 76%|███████▌  | 11072/14596 [1:35:17<22:35,  2.60it/s] 76%|███████▌  | 11080/14596 [1:35:20<22:25,  2.61it/s] 76%|███████▌  | 11088/14596 [1:35:23<22:30,  2.60it/s] 76%|███████▌  | 11096/14596 [1:35:27<22:23,  2.61it/s] 76%|███████▌  | 11104/14596 [1:35:30<22:19,  2.61it/s] 76%|███████▌  | 11112/14596 [1:35:33<22:17,  2.60it/s] 76%|███████▌  | 11120/14596 [1:35:36<22:13,  2.61it/s] 76%|███████▌  | 11128/14596 [1:35:39<22:10,  2.61it/s] 76%|███████▋  | 11136/14596 [1:35:42<22:07,  2.61it/s] 76%|███████▋  | 11144/14596 [1:35:45<22:00,  2.61it/s] 76%|███████▋  | 11152/14596 [1:35:48<21:55,  2.62it/s] 76%|███████▋  | 11160/14596 [1:35:51<22:01,  2.60it/s] 77%|███████▋  | 11168/14596 [1:35:54<21:56,  2.60it/s] 77%|███████▋  | 11176/14596 [1:35:57<21:52,  2.61it/s] 77%|███████▋  | 11184/14596 [1:36:00<21:40,  2.62it/s] 77%|███████▋  | 11192/14596 [1:36:03<21:39,  2.62it/s] 77%|███████▋  | 11200/14596 [1:36:06<21:41,  2.61it/s] 77%|███████▋  | 11208/14596 [1:36:09<21:39,  2.61it/s] 77%|███████▋  | 11216/14596 [1:36:12<21:29,  2.62it/s] 77%|███████▋  | 11224/14596 [1:36:16<21:31,  2.61it/s] 77%|███████▋  | 11232/14596 [1:36:19<21:23,  2.62it/s] 77%|███████▋  | 11240/14596 [1:36:22<21:16,  2.63it/s] 77%|███████▋  | 11248/14596 [1:36:25<21:15,  2.63it/s] 77%|███████▋  | 11256/14596 [1:36:28<21:13,  2.62it/s] 77%|███████▋  | 11264/14596 [1:36:31<21:14,  2.61it/s] 77%|███████▋  | 11272/14596 [1:36:34<21:07,  2.62it/s] 77%|███████▋  | 11280/14596 [1:36:37<21:04,  2.62it/s] 77%|███████▋  | 11288/14596 [1:36:40<21:03,  2.62it/s] 77%|███████▋  | 11296/14596 [1:36:43<20:58,  2.62it/s] 77%|███████▋  | 11304/14596 [1:36:46<20:58,  2.62it/s] 78%|███████▊  | 11312/14596 [1:36:49<20:53,  2.62it/s] 78%|███████▊  | 11320/14596 [1:36:52<20:54,  2.61it/s] 78%|███████▊  | 11328/14596 [1:36:55<20:44,  2.63it/s] 78%|███████▊  | 11336/14596 [1:36:58<20:42,  2.62it/s] 78%|███████▊  | 11344/14596 [1:37:01<20:34,  2.63it/s] 78%|███████▊  | 11352/14596 [1:37:04<20:35,  2.63it/s] 78%|███████▊  | 11360/14596 [1:37:07<20:27,  2.64it/s] 78%|███████▊  | 11368/14596 [1:37:10<20:18,  2.65it/s] 78%|███████▊  | 11376/14596 [1:37:13<20:14,  2.65it/s] 78%|███████▊  | 11384/14596 [1:37:16<20:14,  2.64it/s] 78%|███████▊  | 11392/14596 [1:37:19<20:13,  2.64it/s] 78%|███████▊  | 11400/14596 [1:37:22<20:13,  2.63it/s] 78%|███████▊  | 11408/14596 [1:37:25<20:07,  2.64it/s] 78%|███████▊  | 11416/14596 [1:37:28<20:04,  2.64it/s] 78%|███████▊  | 11424/14596 [1:37:32<19:59,  2.64it/s] 78%|███████▊  | 11432/14596 [1:37:35<20:00,  2.64it/s] 78%|███████▊  | 11440/14596 [1:37:38<19:55,  2.64it/s] 78%|███████▊  | 11448/14596 [1:37:41<19:50,  2.64it/s] 78%|███████▊  | 11456/14596 [1:37:44<19:47,  2.64it/s] 79%|███████▊  | 11464/14596 [1:37:47<19:46,  2.64it/s] 79%|███████▊  | 11472/14596 [1:37:50<19:39,  2.65it/s] 79%|███████▊  | 11480/14596 [1:37:53<19:34,  2.65it/s] 79%|███████▊  | 11488/14596 [1:37:56<19:28,  2.66it/s] 79%|███████▉  | 11496/14596 [1:37:59<19:28,  2.65it/s] 79%|███████▉  | 11504/14596 [1:38:02<19:25,  2.65it/s] 79%|███████▉  | 11512/14596 [1:38:05<19:22,  2.65it/s] 79%|███████▉  | 11520/14596 [1:38:08<19:14,  2.66it/s] 79%|███████▉  | 11528/14596 [1:38:11<19:13,  2.66it/s] 79%|███████▉  | 11536/14596 [1:38:14<19:06,  2.67it/s] 79%|███████▉  | 11544/14596 [1:38:17<19:10,  2.65it/s] 79%|███████▉  | 11552/14596 [1:38:20<19:00,  2.67it/s] 79%|███████▉  | 11560/14596 [1:38:23<19:00,  2.66it/s] 79%|███████▉  | 11568/14596 [1:38:26<19:02,  2.65it/s] 79%|███████▉  | 11576/14596 [1:38:29<18:52,  2.67it/s] 79%|███████▉  | 11584/14596 [1:38:32<18:50,  2.66it/s] 79%|███████▉  | 11592/14596 [1:38:35<18:49,  2.66it/s] 79%|███████▉  | 11600/14596 [1:38:38<18:45,  2.66it/s] 80%|███████▉  | 11608/14596 [1:38:41<18:41,  2.67it/s] 80%|███████▉  | 11616/14596 [1:38:44<18:39,  2.66it/s] 80%|███████▉  | 11624/14596 [1:38:47<18:33,  2.67it/s] 80%|███████▉  | 11632/14596 [1:38:50<18:29,  2.67it/s] 80%|███████▉  | 11640/14596 [1:38:53<18:29,  2.66it/s] 80%|███████▉  | 11648/14596 [1:38:56<18:23,  2.67it/s] 80%|███████▉  | 11656/14596 [1:38:59<18:16,  2.68it/s] 80%|███████▉  | 11664/14596 [1:39:02<18:18,  2.67it/s] 80%|███████▉  | 11672/14596 [1:39:05<18:13,  2.67it/s] 80%|████████  | 11680/14596 [1:39:08<18:07,  2.68it/s] 80%|████████  | 11688/14596 [1:39:11<18:03,  2.68it/s] 80%|████████  | 11696/14596 [1:39:14<18:01,  2.68it/s] 80%|████████  | 11704/14596 [1:39:17<18:00,  2.68it/s] 80%|████████  | 11712/14596 [1:39:20<17:58,  2.67it/s] 80%|████████  | 11720/14596 [1:39:23<17:51,  2.68it/s] 80%|████████  | 11728/14596 [1:39:26<17:45,  2.69it/s] 80%|████████  | 11736/14596 [1:39:29<17:45,  2.68it/s] 80%|████████  | 11744/14596 [1:39:32<17:43,  2.68it/s] 81%|████████  | 11752/14596 [1:39:35<17:40,  2.68it/s] 81%|████████  | 11760/14596 [1:39:37<17:35,  2.69it/s] 81%|████████  | 11768/14596 [1:39:40<17:32,  2.69it/s] 81%|████████  | 11776/14596 [1:39:43<17:33,  2.68it/s] 81%|████████  | 11784/14596 [1:39:46<17:24,  2.69it/s] 81%|████████  | 11792/14596 [1:39:49<17:21,  2.69it/s] 81%|████████  | 11800/14596 [1:39:52<17:14,  2.70it/s] 81%|████████  | 11808/14596 [1:39:55<17:15,  2.69it/s] 81%|████████  | 11816/14596 [1:39:58<17:09,  2.70it/s] 81%|████████  | 11824/14596 [1:40:01<17:11,  2.69it/s] 81%|████████  | 11832/14596 [1:40:04<17:01,  2.71it/s] 81%|████████  | 11840/14596 [1:40:07<17:03,  2.69it/s] 81%|████████  | 11848/14596 [1:40:10<17:01,  2.69it/s] 81%|████████  | 11856/14596 [1:40:13<16:56,  2.70it/s] 81%|████████▏ | 11864/14596 [1:40:16<16:55,  2.69it/s] 81%|████████▏ | 11872/14596 [1:40:19<16:50,  2.70it/s] 81%|████████▏ | 11880/14596 [1:40:22<16:48,  2.69it/s] 81%|████████▏ | 11888/14596 [1:40:25<16:41,  2.70it/s] 82%|████████▏ | 11896/14596 [1:40:28<16:37,  2.71it/s] 82%|████████▏ | 11904/14596 [1:40:31<16:34,  2.71it/s] 82%|████████▏ | 11912/14596 [1:40:34<16:28,  2.72it/s] 82%|████████▏ | 11920/14596 [1:40:37<16:25,  2.72it/s] 82%|████████▏ | 11928/14596 [1:40:40<16:26,  2.70it/s] 82%|████████▏ | 11936/14596 [1:40:43<16:23,  2.70it/s] 82%|████████▏ | 11944/14596 [1:40:46<16:18,  2.71it/s] 82%|████████▏ | 11952/14596 [1:40:49<16:17,  2.71it/s] 82%|████████▏ | 11960/14596 [1:40:51<16:11,  2.71it/s] 82%|████████▏ | 11968/14596 [1:40:54<16:07,  2.72it/s] 82%|████████▏ | 11976/14596 [1:40:57<16:07,  2.71it/s] 82%|████████▏ | 11984/14596 [1:41:00<16:01,  2.72it/s] 82%|████████▏ | 11992/14596 [1:41:03<15:58,  2.72it/s] 82%|████████▏ | 12000/14596 [1:41:06<15:52,  2.72it/s] 82%|████████▏ | 12008/14596 [1:41:09<15:51,  2.72it/s] 82%|████████▏ | 12016/14596 [1:41:12<15:46,  2.73it/s] 82%|████████▏ | 12024/14596 [1:41:15<15:38,  2.74it/s] 82%|████████▏ | 12032/14596 [1:41:18<15:37,  2.73it/s] 82%|████████▏ | 12040/14596 [1:41:21<15:34,  2.74it/s] 83%|████████▎ | 12048/14596 [1:41:24<15:28,  2.75it/s] 83%|████████▎ | 12056/14596 [1:41:27<15:25,  2.74it/s] 83%|████████▎ | 12064/14596 [1:41:30<15:22,  2.75it/s] 83%|████████▎ | 12072/14596 [1:41:32<15:20,  2.74it/s] 83%|████████▎ | 12080/14596 [1:41:35<15:19,  2.74it/s] 83%|████████▎ | 12088/14596 [1:41:38<15:12,  2.75it/s] 83%|████████▎ | 12096/14596 [1:41:41<15:11,  2.74it/s] 83%|████████▎ | 12104/14596 [1:41:44<15:08,  2.74it/s] 83%|████████▎ | 12112/14596 [1:41:47<15:06,  2.74it/s] 83%|████████▎ | 12120/14596 [1:41:50<15:06,  2.73it/s] 83%|████████▎ | 12128/14596 [1:41:53<15:04,  2.73it/s] 83%|████████▎ | 12136/14596 [1:41:56<15:02,  2.73it/s] 83%|████████▎ | 12144/14596 [1:41:59<14:55,  2.74it/s] 83%|████████▎ | 12152/14596 [1:42:02<14:50,  2.75it/s] 83%|████████▎ | 12160/14596 [1:42:05<14:50,  2.74it/s] 83%|████████▎ | 12168/14596 [1:42:08<14:45,  2.74it/s] 83%|████████▎ | 12176/14596 [1:42:10<14:45,  2.73it/s] 83%|████████▎ | 12184/14596 [1:42:13<14:41,  2.74it/s] 84%|████████▎ | 12192/14596 [1:42:16<14:37,  2.74it/s] 84%|████████▎ | 12200/14596 [1:42:19<14:34,  2.74it/s] 84%|████████▎ | 12208/14596 [1:42:22<14:32,  2.74it/s] 84%|████████▎ | 12216/14596 [1:42:25<14:29,  2.74it/s] 84%|████████▎ | 12224/14596 [1:42:28<14:26,  2.74it/s] 84%|████████▍ | 12232/14596 [1:42:31<14:22,  2.74it/s] 84%|████████▍ | 12240/14596 [1:42:34<14:21,  2.73it/s] 84%|████████▍ | 12248/14596 [1:42:37<14:14,  2.75it/s] 84%|████████▍ | 12256/14596 [1:42:40<14:12,  2.75it/s] 84%|████████▍ | 12264/14596 [1:42:43<14:10,  2.74it/s] 84%|████████▍ | 12272/14596 [1:42:45<14:07,  2.74it/s] 84%|████████▍ | 12280/14596 [1:42:48<14:06,  2.74it/s] 84%|████████▍ | 12288/14596 [1:42:51<13:59,  2.75it/s] 84%|████████▍ | 12296/14596 [1:42:54<14:02,  2.73it/s] 84%|████████▍ | 12304/14596 [1:42:57<13:59,  2.73it/s] 84%|████████▍ | 12312/14596 [1:43:00<13:55,  2.73it/s] 84%|████████▍ | 12320/14596 [1:43:03<13:48,  2.75it/s] 84%|████████▍ | 12328/14596 [1:43:06<13:42,  2.76it/s] 85%|████████▍ | 12336/14596 [1:43:09<13:39,  2.76it/s] 85%|████████▍ | 12344/14596 [1:43:12<13:36,  2.76it/s] 85%|████████▍ | 12352/14596 [1:43:15<13:30,  2.77it/s] 85%|████████▍ | 12360/14596 [1:43:17<13:29,  2.76it/s] 85%|████████▍ | 12368/14596 [1:43:20<13:24,  2.77it/s] 85%|████████▍ | 12376/14596 [1:43:23<13:23,  2.76it/s] 85%|████████▍ | 12384/14596 [1:43:26<13:23,  2.75it/s] 85%|████████▍ | 12392/14596 [1:43:29<13:15,  2.77it/s] 85%|████████▍ | 12400/14596 [1:43:32<13:14,  2.76it/s] 85%|████████▌ | 12408/14596 [1:43:35<13:10,  2.77it/s] 85%|████████▌ | 12416/14596 [1:43:38<13:03,  2.78it/s] 85%|████████▌ | 12424/14596 [1:43:41<13:01,  2.78it/s] 85%|████████▌ | 12432/14596 [1:43:43<12:56,  2.79it/s] 85%|████████▌ | 12440/14596 [1:43:46<12:56,  2.78it/s] 85%|████████▌ | 12448/14596 [1:43:49<12:52,  2.78it/s] 85%|████████▌ | 12456/14596 [1:43:52<12:47,  2.79it/s] 85%|████████▌ | 12464/14596 [1:43:55<12:48,  2.77it/s] 85%|████████▌ | 12472/14596 [1:43:58<12:43,  2.78it/s] 86%|████████▌ | 12480/14596 [1:44:01<12:36,  2.80it/s] 86%|████████▌ | 12488/14596 [1:44:03<12:31,  2.80it/s] 86%|████████▌ | 12496/14596 [1:44:06<12:27,  2.81it/s] 86%|████████▌ | 12504/14596 [1:44:09<12:24,  2.81it/s] 86%|████████▌ | 12512/14596 [1:44:12<12:19,  2.82it/s] 86%|████████▌ | 12520/14596 [1:44:15<12:18,  2.81it/s] 86%|████████▌ | 12528/14596 [1:44:18<12:16,  2.81it/s] 86%|████████▌ | 12536/14596 [1:44:20<12:11,  2.82it/s] 86%|████████▌ | 12544/14596 [1:44:23<12:08,  2.82it/s] 86%|████████▌ | 12552/14596 [1:44:26<12:01,  2.83it/s] 86%|████████▌ | 12560/14596 [1:44:29<11:59,  2.83it/s] 86%|████████▌ | 12568/14596 [1:44:32<11:54,  2.84it/s] 86%|████████▌ | 12576/14596 [1:44:35<11:50,  2.84it/s] 86%|████████▌ | 12584/14596 [1:44:37<11:49,  2.83it/s] 86%|████████▋ | 12592/14596 [1:44:40<11:47,  2.83it/s] 86%|████████▋ | 12600/14596 [1:44:43<11:45,  2.83it/s] 86%|████████▋ | 12608/14596 [1:44:46<11:38,  2.84it/s] 86%|████████▋ | 12616/14596 [1:44:49<11:40,  2.83it/s] 86%|████████▋ | 12624/14596 [1:44:51<11:32,  2.85it/s] 87%|████████▋ | 12632/14596 [1:44:54<11:33,  2.83it/s] 87%|████████▋ | 12640/14596 [1:44:57<11:31,  2.83it/s] 87%|████████▋ | 12648/14596 [1:45:00<11:27,  2.83it/s] 87%|████████▋ | 12656/14596 [1:45:03<11:22,  2.84it/s] 87%|████████▋ | 12664/14596 [1:45:06<11:22,  2.83it/s] 87%|████████▋ | 12672/14596 [1:45:08<11:19,  2.83it/s] 87%|████████▋ | 12680/14596 [1:45:11<11:16,  2.83it/s] 87%|████████▋ | 12688/14596 [1:45:14<11:16,  2.82it/s] 87%|████████▋ | 12696/14596 [1:45:17<11:16,  2.81it/s] 87%|████████▋ | 12704/14596 [1:45:20<11:11,  2.82it/s] 87%|████████▋ | 12712/14596 [1:45:23<11:04,  2.83it/s] 87%|████████▋ | 12720/14596 [1:45:25<10:58,  2.85it/s] 87%|████████▋ | 12728/14596 [1:45:28<10:55,  2.85it/s] 87%|████████▋ | 12736/14596 [1:45:31<10:54,  2.84it/s] 87%|████████▋ | 12744/14596 [1:45:34<10:52,  2.84it/s] 87%|████████▋ | 12752/14596 [1:45:37<10:46,  2.85it/s] 87%|████████▋ | 12760/14596 [1:45:39<10:45,  2.85it/s] 87%|████████▋ | 12768/14596 [1:45:42<10:41,  2.85it/s] 88%|████████▊ | 12776/14596 [1:45:45<10:37,  2.86it/s] 88%|████████▊ | 12784/14596 [1:45:48<10:37,  2.84it/s] 88%|████████▊ | 12792/14596 [1:45:51<10:31,  2.86it/s] 88%|████████▊ | 12800/14596 [1:45:53<10:28,  2.86it/s] 88%|████████▊ | 12808/14596 [1:45:56<10:24,  2.86it/s] 88%|████████▊ | 12816/14596 [1:45:59<10:22,  2.86it/s] 88%|████████▊ | 12824/14596 [1:46:02<10:18,  2.87it/s] 88%|████████▊ | 12832/14596 [1:46:05<10:13,  2.88it/s] 88%|████████▊ | 12840/14596 [1:46:07<10:11,  2.87it/s] 88%|████████▊ | 12848/14596 [1:46:10<10:07,  2.88it/s] 88%|████████▊ | 12856/14596 [1:46:13<10:06,  2.87it/s] 88%|████████▊ | 12864/14596 [1:46:16<10:01,  2.88it/s] 88%|████████▊ | 12872/14596 [1:46:18<10:00,  2.87it/s] 88%|████████▊ | 12880/14596 [1:46:21<09:54,  2.89it/s] 88%|████████▊ | 12888/14596 [1:46:24<09:51,  2.89it/s] 88%|████████▊ | 12896/14596 [1:46:27<09:51,  2.87it/s] 88%|████████▊ | 12904/14596 [1:46:30<09:44,  2.89it/s] 88%|████████▊ | 12912/14596 [1:46:32<09:41,  2.90it/s] 89%|████████▊ | 12920/14596 [1:46:35<09:40,  2.89it/s] 89%|████████▊ | 12928/14596 [1:46:38<09:37,  2.89it/s] 89%|████████▊ | 12936/14596 [1:46:41<09:35,  2.88it/s] 89%|████████▊ | 12944/14596 [1:46:43<09:30,  2.89it/s] 89%|████████▊ | 12952/14596 [1:46:46<09:26,  2.90it/s] 89%|████████▉ | 12960/14596 [1:46:49<09:25,  2.89it/s] 89%|████████▉ | 12968/14596 [1:46:52<09:22,  2.90it/s] 89%|████████▉ | 12976/14596 [1:46:54<09:15,  2.92it/s] 89%|████████▉ | 12984/14596 [1:46:57<09:14,  2.91it/s] 89%|████████▉ | 12992/14596 [1:47:00<09:11,  2.91it/s] 89%|████████▉ | 13000/14596 [1:47:03<09:08,  2.91it/s] 89%|████████▉ | 13008/14596 [1:47:05<09:04,  2.92it/s] 89%|████████▉ | 13016/14596 [1:47:08<09:03,  2.91it/s] 89%|████████▉ | 13024/14596 [1:47:11<08:58,  2.92it/s] 89%|████████▉ | 13032/14596 [1:47:14<08:55,  2.92it/s] 89%|████████▉ | 13040/14596 [1:47:16<08:53,  2.92it/s] 89%|████████▉ | 13048/14596 [1:47:19<08:48,  2.93it/s] 89%|████████▉ | 13056/14596 [1:47:22<08:48,  2.92it/s] 90%|████████▉ | 13064/14596 [1:47:24<08:43,  2.93it/s] 90%|████████▉ | 13072/14596 [1:47:27<08:41,  2.92it/s] 90%|████████▉ | 13080/14596 [1:47:30<08:37,  2.93it/s] 90%|████████▉ | 13088/14596 [1:47:33<08:38,  2.91it/s] 90%|████████▉ | 13096/14596 [1:47:35<08:32,  2.93it/s] 90%|████████▉ | 13104/14596 [1:47:38<08:28,  2.93it/s] 90%|████████▉ | 13112/14596 [1:47:41<08:23,  2.94it/s] 90%|████████▉ | 13120/14596 [1:47:44<08:19,  2.96it/s] 90%|████████▉ | 13128/14596 [1:47:46<08:17,  2.95it/s] 90%|████████▉ | 13136/14596 [1:47:49<08:12,  2.96it/s] 90%|█████████ | 13144/14596 [1:47:52<08:11,  2.95it/s] 90%|█████████ | 13152/14596 [1:47:54<08:08,  2.95it/s] 90%|█████████ | 13160/14596 [1:47:57<08:05,  2.96it/s] 90%|█████████ | 13168/14596 [1:48:00<08:02,  2.96it/s] 90%|█████████ | 13176/14596 [1:48:02<08:00,  2.96it/s] 90%|█████████ | 13184/14596 [1:48:05<07:57,  2.96it/s] 90%|█████████ | 13192/14596 [1:48:08<07:57,  2.94it/s] 90%|█████████ | 13200/14596 [1:48:11<07:56,  2.93it/s] 90%|█████████ | 13208/14596 [1:48:13<07:52,  2.94it/s] 91%|█████████ | 13216/14596 [1:48:16<07:50,  2.93it/s] 91%|█████████ | 13224/14596 [1:48:19<07:46,  2.94it/s] 91%|█████████ | 13232/14596 [1:48:22<07:41,  2.96it/s] 91%|█████████ | 13240/14596 [1:48:24<07:41,  2.94it/s] 91%|█████████ | 13248/14596 [1:48:27<07:37,  2.95it/s] 91%|█████████ | 13256/14596 [1:48:30<07:35,  2.94it/s] 91%|█████████ | 13264/14596 [1:48:32<07:31,  2.95it/s] 91%|█████████ | 13272/14596 [1:48:35<07:28,  2.95it/s] 91%|█████████ | 13280/14596 [1:48:38<07:25,  2.95it/s] 91%|█████████ | 13288/14596 [1:48:41<07:23,  2.95it/s] 91%|█████████ | 13296/14596 [1:48:43<07:20,  2.95it/s] 91%|█████████ | 13304/14596 [1:48:46<07:17,  2.96it/s] 91%|█████████ | 13312/14596 [1:48:49<07:13,  2.96it/s] 91%|█████████▏| 13320/14596 [1:48:51<07:11,  2.96it/s] 91%|█████████▏| 13328/14596 [1:48:54<07:07,  2.97it/s] 91%|█████████▏| 13336/14596 [1:48:57<07:03,  2.97it/s] 91%|█████████▏| 13344/14596 [1:48:59<07:01,  2.97it/s] 91%|█████████▏| 13352/14596 [1:49:02<06:57,  2.98it/s] 92%|█████████▏| 13360/14596 [1:49:05<06:54,  2.98it/s] 92%|█████████▏| 13368/14596 [1:49:07<06:52,  2.97it/s] 92%|█████████▏| 13376/14596 [1:49:10<06:49,  2.98it/s] 92%|█████████▏| 13384/14596 [1:49:13<06:47,  2.97it/s] 92%|█████████▏| 13392/14596 [1:49:16<06:45,  2.97it/s] 92%|█████████▏| 13400/14596 [1:49:18<06:42,  2.97it/s] 92%|█████████▏| 13408/14596 [1:49:21<06:40,  2.97it/s] 92%|█████████▏| 13416/14596 [1:49:24<06:35,  2.99it/s] 92%|█████████▏| 13424/14596 [1:49:26<06:32,  2.99it/s] 92%|█████████▏| 13432/14596 [1:49:29<06:27,  3.00it/s] 92%|█████████▏| 13440/14596 [1:49:32<06:26,  2.99it/s] 92%|█████████▏| 13448/14596 [1:49:34<06:23,  3.00it/s] 92%|█████████▏| 13456/14596 [1:49:37<06:18,  3.01it/s] 92%|█████████▏| 13464/14596 [1:49:40<06:16,  3.01it/s] 92%|█████████▏| 13472/14596 [1:49:42<06:13,  3.01it/s] 92%|█████████▏| 13480/14596 [1:49:45<06:11,  3.00it/s] 92%|█████████▏| 13488/14596 [1:49:48<06:09,  3.00it/s] 92%|█████████▏| 13496/14596 [1:49:50<06:07,  2.99it/s] 93%|█████████▎| 13504/14596 [1:49:53<06:02,  3.01it/s] 93%|█████████▎| 13512/14596 [1:49:55<06:00,  3.01it/s] 93%|█████████▎| 13520/14596 [1:49:58<05:57,  3.01it/s] 93%|█████████▎| 13528/14596 [1:50:01<05:54,  3.01it/s] 93%|█████████▎| 13536/14596 [1:50:03<05:51,  3.02it/s] 93%|█████████▎| 13544/14596 [1:50:06<05:47,  3.03it/s] 93%|█████████▎| 13552/14596 [1:50:09<05:44,  3.03it/s] 93%|█████████▎| 13560/14596 [1:50:11<05:46,  2.99it/s] 93%|█████████▎| 13568/14596 [1:50:14<05:39,  3.02it/s] 93%|█████████▎| 13576/14596 [1:50:17<05:38,  3.01it/s] 93%|█████████▎| 13584/14596 [1:50:19<05:34,  3.03it/s] 93%|█████████▎| 13592/14596 [1:50:22<05:30,  3.04it/s] 93%|█████████▎| 13600/14596 [1:50:25<05:28,  3.04it/s] 93%|█████████▎| 13608/14596 [1:50:27<05:26,  3.03it/s] 93%|█████████▎| 13616/14596 [1:50:30<05:23,  3.03it/s] 93%|█████████▎| 13624/14596 [1:50:33<05:21,  3.03it/s] 93%|█████████▎| 13632/14596 [1:50:35<05:18,  3.03it/s] 93%|█████████▎| 13640/14596 [1:50:38<05:15,  3.03it/s] 94%|█████████▎| 13648/14596 [1:50:40<05:12,  3.03it/s] 94%|█████████▎| 13656/14596 [1:50:43<05:07,  3.05it/s] 94%|█████████▎| 13664/14596 [1:50:46<05:06,  3.04it/s] 94%|█████████▎| 13672/14596 [1:50:48<05:03,  3.04it/s] 94%|█████████▎| 13680/14596 [1:50:51<05:00,  3.05it/s] 94%|█████████▍| 13688/14596 [1:50:53<04:56,  3.06it/s] 94%|█████████▍| 13696/14596 [1:50:56<04:54,  3.06it/s] 94%|█████████▍| 13704/14596 [1:50:59<04:51,  3.06it/s] 94%|█████████▍| 13712/14596 [1:51:01<04:48,  3.07it/s] 94%|█████████▍| 13720/14596 [1:51:04<04:45,  3.07it/s] 94%|█████████▍| 13728/14596 [1:51:07<04:44,  3.05it/s] 94%|█████████▍| 13736/14596 [1:51:09<04:39,  3.07it/s] 94%|█████████▍| 13744/14596 [1:51:12<04:38,  3.06it/s] 94%|█████████▍| 13752/14596 [1:51:14<04:34,  3.08it/s] 94%|█████████▍| 13760/14596 [1:51:17<04:31,  3.07it/s] 94%|█████████▍| 13768/14596 [1:51:19<04:28,  3.09it/s] 94%|█████████▍| 13776/14596 [1:51:22<04:25,  3.08it/s] 94%|█████████▍| 13784/14596 [1:51:25<04:22,  3.09it/s] 94%|█████████▍| 13792/14596 [1:51:27<04:19,  3.10it/s] 95%|█████████▍| 13800/14596 [1:51:30<04:16,  3.10it/s] 95%|█████████▍| 13808/14596 [1:51:32<04:13,  3.11it/s] 95%|█████████▍| 13816/14596 [1:51:35<04:11,  3.10it/s] 95%|█████████▍| 13824/14596 [1:51:38<04:08,  3.10it/s] 95%|█████████▍| 13832/14596 [1:51:40<04:05,  3.11it/s] 95%|█████████▍| 13840/14596 [1:51:43<04:03,  3.10it/s] 95%|█████████▍| 13848/14596 [1:51:45<04:00,  3.11it/s] 95%|█████████▍| 13856/14596 [1:51:48<03:57,  3.12it/s] 95%|█████████▍| 13864/14596 [1:51:50<03:54,  3.12it/s] 95%|█████████▌| 13872/14596 [1:51:53<03:51,  3.13it/s] 95%|█████████▌| 13880/14596 [1:51:55<03:48,  3.13it/s] 95%|█████████▌| 13888/14596 [1:51:58<03:46,  3.13it/s] 95%|█████████▌| 13896/14596 [1:52:01<03:43,  3.14it/s] 95%|█████████▌| 13904/14596 [1:52:03<03:40,  3.14it/s] 95%|█████████▌| 13912/14596 [1:52:06<03:38,  3.14it/s] 95%|█████████▌| 13920/14596 [1:52:08<03:35,  3.13it/s] 95%|█████████▌| 13928/14596 [1:52:11<03:33,  3.13it/s] 95%|█████████▌| 13936/14596 [1:52:13<03:30,  3.14it/s] 96%|█████████▌| 13944/14596 [1:52:16<03:28,  3.13it/s] 96%|█████████▌| 13952/14596 [1:52:18<03:25,  3.14it/s] 96%|█████████▌| 13960/14596 [1:52:21<03:22,  3.14it/s] 96%|█████████▌| 13968/14596 [1:52:24<03:20,  3.14it/s] 96%|█████████▌| 13976/14596 [1:52:26<03:17,  3.14it/s] 96%|█████████▌| 13984/14596 [1:52:29<03:14,  3.15it/s] 96%|█████████▌| 13992/14596 [1:52:31<03:11,  3.16it/s] 96%|█████████▌| 14000/14596 [1:52:34<03:08,  3.15it/s] 96%|█████████▌| 14008/14596 [1:52:36<03:05,  3.18it/s] 96%|█████████▌| 14016/14596 [1:52:39<03:01,  3.19it/s] 96%|█████████▌| 14024/14596 [1:52:41<02:59,  3.18it/s] 96%|█████████▌| 14032/14596 [1:52:44<02:57,  3.18it/s] 96%|█████████▌| 14040/14596 [1:52:46<02:54,  3.19it/s] 96%|█████████▌| 14048/14596 [1:52:49<02:51,  3.20it/s] 96%|█████████▋| 14056/14596 [1:52:51<02:48,  3.21it/s] 96%|█████████▋| 14064/14596 [1:52:54<02:45,  3.22it/s] 96%|█████████▋| 14072/14596 [1:52:56<02:43,  3.20it/s] 96%|█████████▋| 14080/14596 [1:52:59<02:40,  3.21it/s] 97%|█████████▋| 14088/14596 [1:53:01<02:38,  3.21it/s] 97%|█████████▋| 14096/14596 [1:53:03<02:35,  3.23it/s] 97%|█████████▋| 14104/14596 [1:53:06<02:32,  3.23it/s] 97%|█████████▋| 14112/14596 [1:53:08<02:29,  3.24it/s] 97%|█████████▋| 14120/14596 [1:53:11<02:27,  3.22it/s] 97%|█████████▋| 14128/14596 [1:53:13<02:24,  3.24it/s] 97%|█████████▋| 14136/14596 [1:53:16<02:22,  3.24it/s] 97%|█████████▋| 14144/14596 [1:53:18<02:19,  3.25it/s] 97%|█████████▋| 14152/14596 [1:53:21<02:16,  3.26it/s] 97%|█████████▋| 14160/14596 [1:53:23<02:13,  3.26it/s] 97%|█████████▋| 14168/14596 [1:53:26<02:11,  3.27it/s] 97%|█████████▋| 14176/14596 [1:53:28<02:09,  3.25it/s] 97%|█████████▋| 14184/14596 [1:53:31<02:06,  3.26it/s] 97%|█████████▋| 14192/14596 [1:53:33<02:03,  3.26it/s] 97%|█████████▋| 14200/14596 [1:53:35<02:00,  3.27it/s] 97%|█████████▋| 14208/14596 [1:53:38<01:58,  3.28it/s] 97%|█████████▋| 14216/14596 [1:53:40<01:55,  3.29it/s] 97%|█████████▋| 14224/14596 [1:53:43<01:53,  3.29it/s] 98%|█████████▊| 14232/14596 [1:53:45<01:50,  3.31it/s] 98%|█████████▊| 14240/14596 [1:53:47<01:47,  3.31it/s] 98%|█████████▊| 14248/14596 [1:53:50<01:45,  3.31it/s] 98%|█████████▊| 14256/14596 [1:53:52<01:42,  3.33it/s] 98%|█████████▊| 14264/14596 [1:53:55<01:39,  3.33it/s] 98%|█████████▊| 14272/14596 [1:53:57<01:36,  3.35it/s] 98%|█████████▊| 14280/14596 [1:53:59<01:34,  3.35it/s] 98%|█████████▊| 14288/14596 [1:54:02<01:32,  3.35it/s] 98%|█████████▊| 14296/14596 [1:54:04<01:28,  3.38it/s] 98%|█████████▊| 14304/14596 [1:54:06<01:26,  3.38it/s] 98%|█████████▊| 14312/14596 [1:54:09<01:24,  3.38it/s] 98%|█████████▊| 14320/14596 [1:54:11<01:21,  3.37it/s] 98%|█████████▊| 14328/14596 [1:54:14<01:19,  3.38it/s] 98%|█████████▊| 14336/14596 [1:54:16<01:16,  3.39it/s] 98%|█████████▊| 14344/14596 [1:54:18<01:14,  3.41it/s] 98%|█████████▊| 14352/14596 [1:54:21<01:11,  3.39it/s] 98%|█████████▊| 14360/14596 [1:54:23<01:09,  3.40it/s] 98%|█████████▊| 14368/14596 [1:54:25<01:06,  3.41it/s] 98%|█████████▊| 14376/14596 [1:54:28<01:04,  3.41it/s] 99%|█████████▊| 14384/14596 [1:54:30<01:02,  3.41it/s] 99%|█████████▊| 14392/14596 [1:54:32<00:59,  3.43it/s] 99%|█████████▊| 14400/14596 [1:54:35<00:56,  3.44it/s] 99%|█████████▊| 14408/14596 [1:54:37<00:54,  3.47it/s] 99%|█████████▉| 14416/14596 [1:54:39<00:51,  3.47it/s] 99%|█████████▉| 14424/14596 [1:54:41<00:49,  3.47it/s] 99%|█████████▉| 14432/14596 [1:54:44<00:47,  3.48it/s] 99%|█████████▉| 14440/14596 [1:54:46<00:44,  3.50it/s] 99%|█████████▉| 14448/14596 [1:54:48<00:41,  3.53it/s] 99%|█████████▉| 14456/14596 [1:54:50<00:39,  3.54it/s] 99%|█████████▉| 14464/14596 [1:54:53<00:37,  3.54it/s] 99%|█████████▉| 14472/14596 [1:54:55<00:35,  3.54it/s] 99%|█████████▉| 14480/14596 [1:54:57<00:32,  3.55it/s] 99%|█████████▉| 14488/14596 [1:54:59<00:30,  3.57it/s] 99%|█████████▉| 14496/14596 [1:55:02<00:27,  3.58it/s] 99%|█████████▉| 14504/14596 [1:55:04<00:25,  3.62it/s] 99%|█████████▉| 14512/14596 [1:55:06<00:23,  3.64it/s] 99%|█████████▉| 14520/14596 [1:55:08<00:20,  3.63it/s]100%|█████████▉| 14528/14596 [1:55:10<00:18,  3.63it/s]100%|█████████▉| 14536/14596 [1:55:13<00:16,  3.67it/s]100%|█████████▉| 14544/14596 [1:55:15<00:14,  3.67it/s]100%|█████████▉| 14552/14596 [1:55:17<00:11,  3.67it/s]100%|█████████▉| 14560/14596 [1:55:19<00:09,  3.72it/s]100%|█████████▉| 14568/14596 [1:55:21<00:07,  3.75it/s]100%|█████████▉| 14576/14596 [1:55:23<00:05,  3.75it/s]100%|█████████▉| 14584/14596 [1:55:25<00:03,  3.79it/s]100%|█████████▉| 14592/14596 [1:55:27<00:01,  3.86it/s]100%|██████████| 14596/14596 [1:55:27<00:00,  2.11it/s]
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/root/.local/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
fatal: not a git repository (or any of the parent directories): .git
{'hellaswag_el': {'alias': 'hellaswag_el', 'acc,none': 0.38786911412609737, 'acc_stderr,none': 0.0048670508134789955, 'acc_norm,none': 0.49012370311252995, 'acc_norm_stderr,none': 0.004993285512116128}, 'hellaswag_hu': {'alias': 'hellaswag_hu', 'acc,none': 0.37372518916547864, 'acc_stderr,none': 0.005066509525346266, 'acc_norm,none': 0.4774646342800746, 'acc_norm_stderr,none': 0.005230926950718493}, 'hellaswag_tr': {'alias': 'hellaswag_tr', 'acc,none': 0.3730704113136142, 'acc_stderr,none': 0.004826562454901856, 'acc_norm,none': 0.47415596056169707, 'acc_norm_stderr,none': 0.004983359616791605}}
